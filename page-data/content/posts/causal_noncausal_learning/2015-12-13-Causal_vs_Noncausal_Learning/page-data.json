{"componentChunkName":"component---src-pages-blog-template-js","path":"/content/posts/causal_noncausal_learning/2015-12-13-Causal_vs_Noncausal_Learning/","result":{"pageContext":{"html":"<p>In many real world problems we are faced with the task of infering the input values (settings) of a system given some expected or observed outcome. A simple example is inferring a set of environment variables from an observed light spectrum: E.g., pressure and temperature can shift and broaden spectral lines<sup id=\"fnref-fn-pressure-bulbs\"><a href=\"#fn-fn-pressure-bulbs\" class=\"footnote-ref\">fn-pressure-bulbs</a></sup> and we are faced with the question what the actural wave-length of the spectral line is. In a complex spectrum lines can start to overlap and hence there are several possibilities for the original line. Problems of this kind are often referred to as <a href=\"https://en.wikipedia.org/wiki/Inverse_problem\">inverse problems</a> and they rarely posses an easy or even single-valued solution.</p>\n<p>For a <a href=\"https://en.wikipedia.org/wiki/Single-valued_function\">single-valued function</a> a given input produces only a single output, i.e. we can describe the relationship with a deterministic relationship:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo>:</mo><mi mathvariant=\"script\">X</mi><mo>→</mo><mi mathvariant=\"script\">Y</mi></mrow><annotation encoding=\"application/x-tex\">f: \\mathcal{X} \\rightarrow \\mathcal{Y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.14643em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7805em;vertical-align:-0.0972em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.08222em;\">Y</span></span></span></span></span></div>\n<p><img src=\"./imgs/single_valued_function.svg\" alt=\"Single Valued Function\"></p>\n<p>This structure implies a sense of causality: Using <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>, we will always observe <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>. However the <em>inverse</em> is not true -- when asking what input leads to <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">y_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> it could be either <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> or <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>. We see that the inverse problem is a <a href=\"http://mathworld.wolfram.com/MultivaluedFunction.html\">multi-valued</a> function. A single value can produce several distinct outcomes and in this sense present a non-causal structure.</p>\n<p>In general, modeling an \"arbitrary\" and complex relation is a formidable task. However, a Data Scientists toolbox contains a powerful companion: Neural Networks that, thanks to their flexibility, are ideally suited. The typical architecture of a <a href=\"http://jotterbach.github.io/2015/11/05/ExploringNeuralNetworkEngineering/\">Feed Forward Neural Network</a> (FFN) consists of a series of layers, each of which processing the (potentially nonlinear) outputs of the previous layer. The flow of information is hence directed from input to output and induces a causal relationship between the input and the output; i.e. they present single-valued functions. This standard architecture breaks down when faced with non-causal, multi-valued relations and to enable Neural Networks to learn such problems, a different output layer is needed that allows the prediction of multiple output values. The corresponding network is called a Mixture Density Model (MDN) and was introduced by <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.120.5685\">Bishop in 1994</a>.</p>\n<p>To understand the differences of these approaches, let's play with a little toy problem with artificially generated data. The <a href=\"https://github.com/jotterbach/SimpleNeuralNets/blob/master/examples/Causal_vs_Noncausal_Learning.ipynb\">IPython notebook</a> and <a href=\"https://github.com/jotterbach/SimpleNeuralNets/tree/master/SimpleNeuralNets\">source code for the network</a> (based on Theano) can be found on my <a href=\"https://github.com/jotterbach\">github account</a>.</p>\n<p>##Learning causal relationship - The Feed-Forward Network architecture</p>\n<p>The artifical data we will use for this post is generated from a simple cubic function</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"double-struck\">r</mi><mo>=</mo><mrow><mo fence=\"true\">(</mo><mtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mi>x</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><msup><mi>x</mi><mn>3</mn></msup><mo>−</mo><mi>α</mi><mi>x</mi></mrow></mstyle></mtd></mtr></mtable><mo fence=\"true\">)</mo></mrow><mo>+</mo><mtext>noise</mtext></mrow><annotation encoding=\"application/x-tex\">\\mathbb{r} = \\begin{pmatrix} x \\\\ x^3 - \\alpha x\\end{pmatrix} + \\text{noise}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.45em;\"><span style=\"top:-3.61em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-2.41em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">3</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord mathnormal\">αx</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.95em;\"><span></span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6679em;\"></span><span class=\"mord text\"><span class=\"mord\">noise</span></span></span></span></span></span></div>\n<p>where the noise is drawn from a Gaussian distribution:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d6e268ecfb4e500dcd7332d8243cbd44/7527b/ToyData.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACkUlEQVR42o1US0sbURSetvRJaRG66GMlKriuuwr+CRtw48aFW/cBySK4qrhKJGIfYOhvcKGQjYs0tVRI0xZtqtN0MncezuPOncljHqfn3kQcpZUOfNxzz/nudx4zd6TFxcVHimK90nWvZJq0gGvxf0GIV7RtDlrQNG9TUeznEgDc6/WA4ArdLiRowxlwD+n95Rh/Dg97cHDQibnteVFGyuVyj1XVJqbJQNPcEBER4kaO40VBwCLMLvbcn0a77UTdLovy+Wa0u0v6/X4IrZb9UtrZ2Xmo665m2x0umOg6Bd9nUKsZUC7/xuynEAQ+UMqAECqgqhR9DPb2NMhmjzDmxZRGoOtORqpWqw/OBFXVTTAIy8s/YGzsA0xOfoTR0RrMzzdEgk6HCaFu1xeis7N1qFR0YMyLXTccCGaz2SeE2IZpdjg5KRRkQWw0LKyYQrVqwNLSd5iYqEEm04BiUYZS6RdMT3+GtbVjTOIDth8zNqwQZ3kHW9V8vwf7+0aysPCVVypa5II4S3GoXj+FfP4nzMwcoNgn2NiQeauCg+fPBbHCZ+22bSRJCCsrzWR7m4iZ8ZYMg4oDfG6OwyCKApHI8wZt85hheChEY9+PcfUyUqXy5b5lUSLLDHK5owS/RUHkLycN7uOVDyoaJDmPpSpcX38/YpqufnLCRMu8xb8JXoULgqurq48IcVTL8gEr7aNYiKQrwb/DS76e4/QAu5sVNyUIgIQh4OuPkyBIcIYwRNpO+y5yfB/ETcF9Rpqbm3vabCrrpulvttvWa1kmWzjojVbLeHd8rJbxBpXQ/wbtLW6j/60sq4LD+dzP/wOKclqu17+9kKampm5KksTBn2sp+zri1tC+ixgZ2jf+xR8fH7/9BxYYjfiQkq/FAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Toy data showing a causal relation\"\n        title=\"\"\n        src=\"/static/d6e268ecfb4e500dcd7332d8243cbd44/fcda8/ToyData.png\"\n        srcset=\"/static/d6e268ecfb4e500dcd7332d8243cbd44/12f09/ToyData.png 148w,\n/static/d6e268ecfb4e500dcd7332d8243cbd44/e4a3f/ToyData.png 295w,\n/static/d6e268ecfb4e500dcd7332d8243cbd44/fcda8/ToyData.png 590w,\n/static/d6e268ecfb4e500dcd7332d8243cbd44/7527b/ToyData.png 754w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h3>Causal relation</h3>\n<p>The structure of the data is simple with a clear input and target variable. To model the relationship we will train a FFN with a single input and output neuron and one hidden layer containing 25 neurons with a <code class=\"language-text\">tanh</code> activation. During training we record the loss of the training as well using a similarly generated training set.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/36563e8f3d4899a57b18a3845bf8ecc6/8ae3e/TrainTestLossesSimpleFFN.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.91891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAABt0lEQVR42p2TSU7DMBSGI5DYciVWrNlxAUACBGJPQWIUEjdAnIIFV2DJgk2RwpSpie2kmdrYiZvHsxsqKBFQnvQptuN8/fNSG4ZhzN3d3S/FsbgIw8ExcvIjdHBCaTbF4Mh12aXv91cMVbmo1wErK0Yy5zW0UZQAogIoJcBoBCDlGFHWkBa1jJMCoii71sLHx9edJC3BD2KOVG04vbCyPFa9ebSy8UpIOiEgCU8SAUEQX2mh59BNXgCQIJGUpNAKzSbYXgj4A3qMQhQlUgjAa5PQQWFRgL6hNvwGQxEmnczVc5zr58fCbvdlbzj8u5CSDFw/As/v6/E3YRjma7MkVBIlw17q1/4mNE17Vy3gZvm5V7/x6pKPPuoe4ngspDTemCUhaT6S3WM6KZlOaFnB9n+EHvZR9ZLRXJYq4cdXtixvK8ukEgolbYOQ6XkqMZ18sn355hLRTzkENBn/D0VzUjivdS9mIR+OIB2oMwMQZUOdcP7m5na51+ufMZbuIx3bDo6fnx017xASHyhM0zn3/ehQjdW669Ij07TOGcs7jOQdPOOnDw/dVaOpBeNrzTV8rsWptbY9C+8894lPxgvWAwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Train- and test-set losses\"\n        title=\"\"\n        src=\"/static/36563e8f3d4899a57b18a3845bf8ecc6/fcda8/TrainTestLossesSimpleFFN.png\"\n        srcset=\"/static/36563e8f3d4899a57b18a3845bf8ecc6/12f09/TrainTestLossesSimpleFFN.png 148w,\n/static/36563e8f3d4899a57b18a3845bf8ecc6/e4a3f/TrainTestLossesSimpleFFN.png 295w,\n/static/36563e8f3d4899a57b18a3845bf8ecc6/fcda8/TrainTestLossesSimpleFFN.png 590w,\n/static/36563e8f3d4899a57b18a3845bf8ecc6/8ae3e/TrainTestLossesSimpleFFN.png 756w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>As a learning objective I chose to minimize the standard squared-error loss. Using normalized gradient descent the loss function is clearly decreasing with increasing number of steps, but it is also interesting to see that there are very distinct points where the slope of loss curve significantly changes before entering another region of steep descent (Note that the plot is in log-log scale). A potential explanation for this behavior is cited in <a href=\"http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html\">LeCun et al., 2015</a>:</p>\n<blockquote>\n<p>In practice, poor local minima are rarely a problem with large networks. Regardless of the initial conditions, the system nearly always reaches solutions of very similar quality. Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general. Instead, the landscape is packed with a combinatorially large number of saddle points where the gradient is zero, and the surface curves up in most dimensions and curves down in the remainder</p>\n</blockquote>\n<p>The <a href=\"http://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html\">basic intuition</a> behind this phenomenon lies in the fact that in high dimensions it becomes increasingly unlikely that the loss surface curves upwards, i.e. the loss increases, in all direction we could choose to go <sup id=\"fnref-fn-loss-surface\"><a href=\"#fn-fn-loss-surface\" class=\"footnote-ref\">fn-loss-surface</a></sup>. In reverse, it is increasingly likely that there is another direction in which minimization can occur, and gradient decent is likely to find it (though it can get stuck).</p>\n<p>After seeing that the training seems to work let's have a look at the actual prediction plotted with the training data:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/45d8333214080a7a64510194acbfbc67/7527b/PredictionSimpleFNN.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACcklEQVR42pVUy27TQBQ1IEA8NogNlVj2C8iOv4igX1F1y6KbfgDbFhU1FCkRAqkrEBIP8RCvVlBRQVviNCJNjGt77BnbSfzIy/HhTuS+KEjF0tFcHY/PPXfuHSuTk5OXDcO77TjBvBDtWVrn/gsinGNOe9a2WwuG4V9TAJzv9cBoRbeLlGIcG32gHycwG9ZQfk/chLK5uXmR1E0hQtA6ICTHAmsm3IuTXxvVpPJlvR9EKUxT3FSmp6fHGPO573ekYOo4bTh2C9wNIZodODzAiPsTtEd4ESrvVmBo9jCKU+KbE4plWRdIyN4TlGJeDLthQf+uwt4RJNyV70CusJ8wgl6pY+vDZwi/Mwzag78IsmYqSExfXYdaKKJWfIgKof7mE1xy47hxJirdxSg/fQm2bYAaMwyCTHBmZubKqGSPynPD1KpqKC89hq078MiFUzdRe/IcW5Ir1yB4CI/Eq6/eo/GWEpF7xprDMEyOOuQiTNVnr2HUjL0yKTvcVg/62g+Ui4/w88ES1MUSJXkBQecsj4D27QvKsaESWCtIoJVrafXj6ijrSCxrgIwlJ0vd+abCWt+CnIrdhh0SnJq6ddWyPC4F1eWvqdlg0umRrnKZgAQ4CXPqPpfcfsLDJTsiMEzqZmV5rU+ZB3Qmch4H5GiQzeaBmN7RDO7yGXq+L6tq39i7KdKBYH7apRsQ0ZDGMQi760GkR/gowuimUDyh5PP5se1t405dsxZ2TLegaazIeXBX1/n9et0qkeN50/TuUVyUMfGLmmaN9hBfkLz8DxiGW9rYUK8ruVzutKIoEvI5cSA+STiTxecIl7L41L/2j4+Pn/0N26aiAuH3hZoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Prediction of a simple FFN for a cubic curve\"\n        title=\"\"\n        src=\"/static/45d8333214080a7a64510194acbfbc67/fcda8/PredictionSimpleFNN.png\"\n        srcset=\"/static/45d8333214080a7a64510194acbfbc67/12f09/PredictionSimpleFNN.png 148w,\n/static/45d8333214080a7a64510194acbfbc67/e4a3f/PredictionSimpleFNN.png 295w,\n/static/45d8333214080a7a64510194acbfbc67/fcda8/PredictionSimpleFNN.png 590w,\n/static/45d8333214080a7a64510194acbfbc67/7527b/PredictionSimpleFNN.png 754w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Fits pretty well to the data! The minimization objective clearly worked, with some exception at the edges around where the input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo>=</mo><mo>±</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=\\pm 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">±</span><span class=\"mord\">1</span></span></span></span></span>. Here the data is simply too sparse for the network to learn good values. This is a known effect as Neural Networks need enough data to learn good representations of the data. What might not be very well visible is that the deviations from the model become increase the further we extrapolate the predictions to regions where no training data was available. This problem is not specific to Neural Networks, but rather a generic problem of Machine Learning algorithms.</p>\n<p>It should be pointed out that the used network is not a \"deep\" network as it contains only a single layer with only 25 neurons (and I saw similar results with even less neurons). Nevertheless the data fits remarkably well. (Now I'm curious to see how it compares to a Random Forest Regressor with limited depth. Maybe a topic of another post ...)</p>\n<h3>Non-causal relation with a Feed-Forward Network</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7464f023a89c9be72502471f70fed72f/7527b/NonCausalRelation_Data.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACsklEQVR42oVUz0vcQBQOQpH+AGkPpdKje/JYbz3oxasn68m/wcOCXvagIMWDiyKopa7dS7cnUU+CtyILXVQQZF0UrTH+SMwku9n8mMRNNrN5fZNFu3aFDnxMJvnme9978yZCMpl8oyjVtK7Tr7ruLOG8/BQIsZdVtQnTpMu+7z7Atp2lcpmuKor5QVBV9aXvNzTA4fsQBQHAPXANtRoAYxCPKGrCNAEuLhicndXh4MCH8/N6g3/HPSNCOp1+q6omqVRc0DQ7RDAOdMQMw2FB4DJZttj6uszm5kQ2OyuyxcVLls1es40NmRUKOpMks25ZAe43PwmlUukVCmimWeOCEaYNHJWKgy5MmJ4WYWDgECYmfsPWlgrHx1VQFAtc1wVKXbAsily74boM91kjQir1uZsQs9wqqGlOTB4bOwVB+AkLC5eYUA1s28V0aRyMcwhpBsd9fwWfcsjJ1SqF7W0CPT17MDh4CKJoxm4UxY6/l8tNsTbBqampd/865ODReVrFYhX6+w8hkdiHtTU5dtgq1ibIT/mpGnJgi8Tp2TaF8fEz6Oz8BZOT4iOxNsFkMvVe0yydC+LJRjwdDk40DBqLRdEdnJ6a0NWVh97efZAkCwPRB16bQ3xJbNvHGjkRPwwuwmsoyzYUCmVYXb2GoaEjGB4uwc6OHge6F2sTTKVS3eiQ3Nw4sLtbrufzWri5KbNM5iqcmRHD+XkpzOVuwmLRCH3fC12XYq86IdYxRLF7BLwPsTzDAvbDC7wNxLIAu96L9vY8ODkJMBrDQ4mg0WjeEj57XhS/u7uDR/A8iFn4PCKMjo52i6LyBVP85nk0gy4yqnr7gxAji/dzRVXpytWVkZUk8p2vMb0MnyVJzd3eNjn8P6AoRu7o6OSj0NfX90wQBI7WwdcdLevniNf/4XQkEonOP1+hiE4ZIKmpAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Multivalued input-output relation\"\n        title=\"\"\n        src=\"/static/7464f023a89c9be72502471f70fed72f/fcda8/NonCausalRelation_Data.png\"\n        srcset=\"/static/7464f023a89c9be72502471f70fed72f/12f09/NonCausalRelation_Data.png 148w,\n/static/7464f023a89c9be72502471f70fed72f/e4a3f/NonCausalRelation_Data.png 295w,\n/static/7464f023a89c9be72502471f70fed72f/fcda8/NonCausalRelation_Data.png 590w,\n/static/7464f023a89c9be72502471f70fed72f/7527b/NonCausalRelation_Data.png 754w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>The story changes dramatically when the data we are trying to learn has not such the nice property of a one-to-one or many-to-one mapping, such as single-valued functions. Exchanging the input and target variable, the resulting relation is now multi-valued. E.g. at input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">x=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0</span></span></span></span></span> we find (at least) three values of the target variable.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f447538c9c065d05774e7600dd96ddaf/7527b/FFN_prediction_multivaluedData.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACxElEQVR42nVTy27TQBQ1IEA8NrBAVGz7BXTHb0BX/Ypss+mGXcQGNahVC4hWIKSqCKkIiQWrCpBKQaV10uDESd3EsT2P2LUd18nYvtxJUppCa+loZnzvPXPuY5RcLnfbNDsFQoJ5Qvw5XItn47BIbLdIeFh0Q1Hkh/GYzZ+jNFg0Tfe+YlnWjThOHcAvjiHr9QDORIrIAPpRCk6jDaxJQPQxpjeIS2U8+k0rhULhjmW5NmMhOM6hQCSnwMLE6UQJUasJ+/Yz0Tc2k9qX74m2XUnqdTPBuMS2vb7n9TDefaSoqnoTAx3XPZKEGcqHv8BLCKqh7z8C/boF1LChXNaBdCKgaCuVajJGIg3DBGO8aSWffzxh2y79jxAdid8H9u4D8LdrwDC1lslB0w5AZsN4FxoNC2q1ljynQSCGhFIhFtzp4K3/ElK8xNxSYefJUyC2B3rdRJI2cCSTyqSfVIwlO1E4Ozt7Fwmpgek0mzSjNBw4ygDOAtD2UcX6J9CXXsHvSgMsJKZ0ZEfifbSj6jSKMiRGQtlldHAkSamkZ8e34xgAlgL2sE4dTKf8ZhWqr1eBufGwHKNMJHm5XD9JOZfL38NA4vsCKpVG1sRxYKiSIeGB4UC12gR+BEBVDfjcwiB1grYTwmDQlCAYpSwVosH2vBja7U4mOye7y/Fc0Qww921gWCe2tg5kc2fQ4VOTMCzPeJfzE3IOZe04D/u63hKaWhXtsi72Pm8Itrkt6A9VkBYTxI0FGc0qEokx9OQcMuY/VHDAr0cR2EIAhGGa4dSDjYU2flUgRDVHeI4ShFy7GaAvyAYM1yG63eFLwf20MjMzM6Hr5jPGuouY8hLO1jLz4gWT+C8bhrMi33jb5M8H/1k4j5PwwjCsZazdwrG/9DFNvrK7u/dAmZqauqwoioT8LoztLyKujPbXELdG+0vn+U9OTl79A1qdnciwcXGCAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"A FFN trying to approximate a multivalued function\"\n        title=\"\"\n        src=\"/static/f447538c9c065d05774e7600dd96ddaf/fcda8/FFN_prediction_multivaluedData.png\"\n        srcset=\"/static/f447538c9c065d05774e7600dd96ddaf/12f09/FFN_prediction_multivaluedData.png 148w,\n/static/f447538c9c065d05774e7600dd96ddaf/e4a3f/FFN_prediction_multivaluedData.png 295w,\n/static/f447538c9c065d05774e7600dd96ddaf/fcda8/FFN_prediction_multivaluedData.png 590w,\n/static/f447538c9c065d05774e7600dd96ddaf/7527b/FFN_prediction_multivaluedData.png 754w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Feeding this data into the FFN of the previous section leads to terrible results. This should not be surprising as the network is trying hard to fit a single valued function to a multivalued target. The root cause of this lies in the architecture of the network where the output layer is trying to predict the exact value of the target variable. Since there is only a single output, there is little hope of being able to approximately even reasonably close.</p>\n<p>This is often referred to as learning causal relationships. A given input (action) leads to a single, predictable outcome (result) and hence there is intrisic causality in the architecture of a simple Feed-Forward Network. To learn Non-Causal relationships we need to modify the way the output layer models the target data. Let's look at that now.</p>\n<p>##Learning a Non-Causal Relationship -- the Mixture Density Model</p>\n<p>Despite the failure of the previous section, the findings give us an idea of how to proceed. Instead of predicting (with certainity) a specific target value <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span> given some input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span>, let's try and predict a probability <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(t \\vert \\mathbb{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span> of a given target value. E.g. if the data looks like it has some Gaussian noise the model would read</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"script\">N</mi><mo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\">(</mo><mi>t</mi><mo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\">∣</mo><mi>μ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\">)</mo><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">p(t \\vert \\mathbb{x}) = \\mathcal{N} \\big(t \\big\\vert \\mu(\\mathbb{x}), \\sigma(\\mathbb{x}) \\big).</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.212em;vertical-align:-0.35em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span><span class=\"mord\"><span class=\"delimsizing size1\">(</span></span><span class=\"mord mathnormal\">t</span><span class=\"mord\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.862em;\"><span style=\"top:-2.256em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.854em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span style=\"height:0.016em;width:0.3333em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"0.3333em\" height=\"0.016em\" style=\"width:0.3333em\" viewBox=\"0 0 333.33000000000004 16\" preserveAspectRatio=\"xMinYMin\"><path d=\"M145 0 H188 V16 H145z M145 0 H188 V16 H145z\"></path></svg></span></span><span style=\"top:-2.862em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span></span><span class=\"mord mathnormal\">μ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"delimsizing size1\">)</span></span><span class=\"mord\">.</span></span></span></span></span></div>\n<p>The task of the FFN is now to learn the parameters <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mu(\\mathbb{x}), \\sigma(\\mathbb{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">μ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span> of the distribution. Drawing from the distribution at a given point <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{x}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span> then gives us an estimate of the probability a given target value will occur.</p>\n<p>This idea can now be easily generalized to multivalued functions! Given the data from above we could argue that in fact we have three distinct distribution that overlap to build a new, more complex distribution and hence the probaility of a given target value <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span> for an input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{x}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span> is then given by</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mi>i</mi><mi>K</mi></munderover><msub><mi>π</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(t \\vert \\mathbb{x}) = \\sum_i^K \\pi_i(\\mathbb{x}) p_i(t \\vert \\mathbb{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.106em;vertical-align:-1.2777em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span></div>\n<p>where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\pi_i(\\mathbb{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span> are the mixture coefficients with <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><msub><mi>π</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\sum_i \\pi_i(\\mathbb{x}) = 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0497em;vertical-align:-0.2997em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.162em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span> is the number of components in the mixture. Intuitively we can understand the final distribution as follows: For a given input <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{x}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span> the probability of obtaining the target value <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6151em;\"></span><span class=\"mord mathnormal\">t</span></span></span></span></span> is given by <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(t \\vert \\mathbb{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span>. In this way the FFN is now able to learn several values at once by not giving a definite answer but rather giving a probability for certain values of the target variable.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1e3c54de743781bf9dcfd018e162ff22/940c5/TrainTestLossMDN.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.89189189189189%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAACHUlEQVR42oWSzW4TMRDHN4AIoF6QEGpQlVxyypNEueUWKZVyi3iB9knyAIinqCqqFiIOHLhAcoEKJZu1d23vrr0f+d7VMN5N0q9QLP0047H9n/HYhmEYhUajcUhs+ZFQ+c2i8gvSd5ygz3nQFyJH+/twEDz7+fra/D4aOW1DCzabzSOpVoP5AsCXyxR98OQSXH+p5xBGa4jj5AFTJIhWwP1ZwlgAQsSnmWC73S5xHv6QcgFoV4jekEGpSkzL3833gfuX02kK6J9kgp1O5xDLH4ThSgdT3ABbhIhgQhTcjt1Hi85mgDY83V3534IhEFsBoSrzdYL/CRaPj9+/ZUwN7wsKHmUCrojBIgFWKsHBXrk8elTwZbfbfWMSPuR+DCYV6cR2QUMcD6jj55b5mZiJotRRmCRPpMHEyXx+I2j0er0DzoJhFKyBszAVmwocpsBmMmNiCxhRB35PCPwaUyDM08lhTDnCkmi6BrERfNJqtd7tvbKIHuC6MYzNvFpdtcay3STGV94KZq+MQj9vf5u7BBvCBEUTi8jkz9jD7yTRVxiP7nybp7pCbOoAcKBNdT8eY7HIbRiloMIEuLdM9FmM51eu1+tHk4n4wFh0hVnOCfEubVt9worOKfUv9Nx1p2d6blnu1XaNOeqCUu8Se36Gsa+jEWkbKFwol8uvK5XKC+NmvNKt2PhF5Ln+DcizUqm0W6tWq0XNZl+hVqsd/AWf4jNa3RyFZwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Training and test losses of the MDN\"\n        title=\"\"\n        src=\"/static/1e3c54de743781bf9dcfd018e162ff22/fcda8/TrainTestLossMDN.png\"\n        srcset=\"/static/1e3c54de743781bf9dcfd018e162ff22/12f09/TrainTestLossMDN.png 148w,\n/static/1e3c54de743781bf9dcfd018e162ff22/e4a3f/TrainTestLossMDN.png 295w,\n/static/1e3c54de743781bf9dcfd018e162ff22/fcda8/TrainTestLossMDN.png 590w,\n/static/1e3c54de743781bf9dcfd018e162ff22/940c5/TrainTestLossMDN.png 772w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>To learn the distribution we need an optimization criterion that tells the network what \"good\" learning means. The default starting point is maximizing the <a href=\"https://en.wikipedia.org/wiki/Likelihood_function\">likelihood function</a> <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"script\">L</mi><mo stretchy=\"false\">(</mo><mi>π</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>μ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}(\\pi(\\mathbb{x}), \\mu(\\mathbb{x}), \\sigma(\\mathbb{x}) \\vert t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathcal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">μ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">t</span><span class=\"mclose\">)</span></span></span></span></span>. However, to use gradient descent and other stable optimization problems it turns out that minimizing the negative log-likelihood<sup id=\"fnref-fn-square-error-loss\"><a href=\"#fn-fn-square-error-loss\" class=\"footnote-ref\">fn-square-error-loss</a></sup> is more optimal, i.e.</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>J</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"double-struck\">w</mi><mtext>opt</mtext></msub><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mrow><mi>min</mi><mo>⁡</mo></mrow><mi mathvariant=\"double-struck\">w</mi></munder><mo fence=\"false\" stretchy=\"true\" minsize=\"2.4em\" maxsize=\"2.4em\">[</mo><mo>−</mo><munderover><mo>∑</mo><mi>n</mi><mi>N</mi></munderover><mi>ln</mi><mo>⁡</mo><mo fence=\"false\" stretchy=\"true\" minsize=\"2.4em\" maxsize=\"2.4em\">(</mo><munderover><mo>∑</mo><mi>i</mi><mi>K</mi></munderover><msub><mi>π</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"double-struck\">x</mi><mi>n</mi></msub><mo separator=\"true\">,</mo><mi mathvariant=\"double-struck\">w</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"script\">N</mi><mo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\">(</mo><msub><mi>t</mi><mi>n</mi></msub><mo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\">∣</mo><mi>μ</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"double-struck\">x</mi><mi>n</mi></msub><mo separator=\"true\">,</mo><mi mathvariant=\"double-struck\">w</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msub><mi mathvariant=\"double-struck\">x</mi><mi>n</mi></msub><mo separator=\"true\">,</mo><mi mathvariant=\"double-struck\">w</mi><mo stretchy=\"false\">)</mo><mo fence=\"false\" stretchy=\"true\" minsize=\"1.2em\" maxsize=\"1.2em\">)</mo><mo fence=\"false\" stretchy=\"true\" minsize=\"2.4em\" maxsize=\"2.4em\">)</mo><mo fence=\"false\" stretchy=\"true\" minsize=\"2.4em\" maxsize=\"2.4em\">]</mo></mrow><annotation encoding=\"application/x-tex\">J(\\mathbb{w}_\\text{opt}) = \\min_\\mathbb{w}\\bigg[ -\\sum_n^N\\ln\\bigg(\\sum_i^K \\pi_i(\\mathbb{x}_n,\\mathbb{w}) \\mathcal{N} \\big(t_n \\big\\vert \\mu(\\mathbb{x}_n,\\mathbb{w}), \\sigma(\\mathbb{x}_n,\\mathbb{w}) \\big)  \\bigg)\\bigg]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">opt</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6679em;\"><span style=\"top:-2.4em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02691em;\">w</span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\">min</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"delimsizing size3\">[</span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.106em;vertical-align:-1.2777em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.9em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.25em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\">ln</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"delimsizing size3\">(</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span><span class=\"mord\"><span class=\"delimsizing size1\">(</span></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.862em;\"><span style=\"top:-2.256em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.854em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span style=\"height:0.016em;width:0.3333em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"0.3333em\" height=\"0.016em\" style=\"width:0.3333em\" viewBox=\"0 0 333.33000000000004 16\" preserveAspectRatio=\"xMinYMin\"><path d=\"M145 0 H188 V16 H145z M145 0 H188 V16 H145z\"></path></svg></span></span><span style=\"top:-2.862em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.35em;\"><span></span></span></span></span></span></span><span class=\"mord mathnormal\">μ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">n</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"delimsizing size1\">)</span></span><span class=\"mord\"><span class=\"delimsizing size3\">)</span></span><span class=\"mord\"><span class=\"delimsizing size3\">]</span></span></span></span></span></span></div>\n<p>Here we explicitly wrote out the weights <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">w</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{w}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span> of the network and the sum over all <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> training samples. Note that it \"seems\" as if the loss should always be positive as the mixture coefficients <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\pi_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> are always summing up to unity and the Normal distribution is normalized. However, for a very narrow Normal distribution there can be values much larger than <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span> which makes the sum larger than <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span> and hence the negative log is negative. A typical form of the loss-minimization with learning progress is shown in the above figure.</p>\n<p><img src=\"./imgs/MDN_architecture.svg\" alt=\"Architecture of MDN output layer\"></p>\n<p>Finally there is one more technicality we need to solve before trying to learn the multi-valued function from before: We need to connect the output of the network to the parameters of the mixture model. For a model with <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span> Normal components and an <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span></span>-dimensional target we have <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span> mixtures and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span> variances (we assume that the variance is the same in all dimensions; however it is easy to generalize the problem using a covariance matrix) and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mo>⋅</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">K\\cdot m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span></span> mean values. In total number of output nodes of the network is then <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mo stretchy=\"false\">(</mo><mi>m</mi><mo>+</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">K(m+2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">m</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">2</span><span class=\"mclose\">)</span></span></span></span></span>. The outputs of the network are real valued and unbounded, and in order to restrict the mixture coefficients and variances to their respective support we need apply a final transformation of the network's output values <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>a</mi><mi>μ</mi></msup><mo separator=\"true\">,</mo><msup><mi>a</mi><mi>σ</mi></msup><mo separator=\"true\">,</mo><msup><mi>a</mi><mi>π</mi></msup></mrow><annotation encoding=\"application/x-tex\">a^\\mu, a^\\sigma, a^\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8588em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">μ</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">σ</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span></span></span></span></span></span></span></span></span>, according to</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>μ</mi><mrow><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow></msub><mo>=</mo><msubsup><mi>a</mi><mrow><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi></mrow><mi>μ</mi></msubsup><mspace linebreak=\"newline\"></mspace><msub><mi>σ</mi><mi>j</mi></msub><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=\"false\">(</mo><msubsup><mi>a</mi><mi>j</mi><mi>σ</mi></msubsup><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><msub><mi>π</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=\"false\">(</mo><msubsup><mi>a</mi><mi>j</mi><mi>π</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mrow><munderover><mo>∑</mo><mi>i</mi><mi>K</mi></munderover><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=\"false\">(</mo><msubsup><mi>a</mi><mi>i</mi><mi>π</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\mu_{j,k} = a^\\mu_{j,k} \\\\\n\\sigma_j = exp(a^\\sigma_j) \\\\\n\\pi_j = \\frac{exp(a^\\pi_j)}{\\sum_i^K exp(a^\\pi_i)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2197em;vertical-align:-0.4374em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7823em;\"><span style=\"top:-2.3987em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span style=\"top:-3.1809em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">μ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4374em;\"><span></span></span></span></span></span></span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1331em;vertical-align:-0.3831em;\"></span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">x</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7144em;\"><span style=\"top:-2.453em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">σ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3831em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.7057em;vertical-align:-1.1709em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5348em;\"><span style=\"top:-2.1288em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9812em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">x</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6462em;\"><span style=\"top:-2.4231em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.0448em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2769em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.7848em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">x</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">a</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6644em;\"><span style=\"top:-2.4413em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3948em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1709em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span></div>\n<p>The last equation for the mixture coefficient <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\pi_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> is called the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\">softmax function</a> and is also often used as an activation function for neurons in the hidden layer of a network.</p>\n<p>With all this we are finally ready to tackle the learning problem of the \"simple\" multi-valued function from above. Letting the network run for 100k iterations we get a very good approximation of our data:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c1b10992c229ed443f023e61f4b2021a/a242d/Trained_MDN.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.89189189189189%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAACgUlEQVR42oWTb0/TQBzHeeADX5BvwFcgEB+IRF+LhkSeKOEJiQFiDCJT+TNhJCCBBQXGnw3YBCfZxv6V9drrre21Xdfutv68tkxFVJp8c9f0d5/7/r7X6+nhjyiq44piWhgbF1zoiiSKJC5Nt5HecJFQROh7KocOoutoN7KIzlYXUfVzrFbZiunV7eV1n9fDF8x1OgCqarc1rQm/i1IXLLsFhSyG1Ykt2BidgczKJuSPCoDyRcDn5yAXSh0TET6/SAdARTFmmk0A7qgly9TrCmPqCVXizU9lvTfDq97hRsaTkObVtQaX7eF681J227Q9UIh97PNuVSpy1HUDYJu3DqEsqGsGLI3vwtiTJMgKBWq5vMYELFMuIxwxBVnWO6bJ/Hm62/I7xwmAjAtCcSjRYXPpFF4+S0O1KAYbyRKFXzWheDdXgbVafdYH8g/tP4sxofBh4gSmnyc4UA+c3wisVvH834Ayb4sQE5CoweTwAcQiZ8EGNwL5wsj1ln8WA6lbUMqLMDlyAutzx6BgLczyH8DbgoAXrh/KZfiSHuTWaDoQfZuHqadxkEXMN2kEsKAOGx3Lavtj1yENHcqU+RDflV/Ifw1QDReIasPO2hmMDe3DUaLEYeb/WxZq2nvL5kBiO0rdZkRtMlQjrHBaZInIClt4scyWp1PsPCczVbMYj4hdxtOVq+sON2GkwlM+TMw6+Qyg/Xi7vBWH07VtyKwl4eTTDuS+JEGqEHBcDxqNNlC9BYZxVfw2dRgDPrZCh5WPI3eSr4cGNl+NPvwam+87jkZ7U/H4YKmGB3gS94jh9O/tpR8LAunnLnrLZfF+Ov3tkSRpff57Nlt4kMuVB3lMd38AT0V0yY9kTj0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Trained MDN\"\n        title=\"\"\n        src=\"/static/c1b10992c229ed443f023e61f4b2021a/fcda8/Trained_MDN.png\"\n        srcset=\"/static/c1b10992c229ed443f023e61f4b2021a/12f09/Trained_MDN.png 148w,\n/static/c1b10992c229ed443f023e61f4b2021a/e4a3f/Trained_MDN.png 295w,\n/static/c1b10992c229ed443f023e61f4b2021a/fcda8/Trained_MDN.png 590w,\n/static/c1b10992c229ed443f023e61f4b2021a/a242d/Trained_MDN.png 724w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>The MDN does exactly what we wanted it to do: It is very narrow in the regions where the data is single valued indicating a high certainty. In the regions where the data is multivalued and broad the distribution has multiple peaks and gives almost equal probability to the three values.</p>\n<p>With this we can also understand what the simple FFN was trying to learn when faced with the multi-valued input: It tried the best possible approximation given all the inputs, which is given by the conditional average of the probability distribution <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msubsup><mo>∑</mo><mi>i</mi><mi>K</mi></msubsup><msub><mi>π</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"double-struck\">x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(t \\vert \\mathbb{x}) = \\sum_i^K \\pi_i(\\mathbb{x}) p_i(t \\vert \\mathbb{x})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2809em;vertical-align:-0.2997em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9812em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">t</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span>. However, the average of several solution is not necessarily a solution itself and hence in general fails to approximate the data.</p>\n<h3>What's the take-away?</h3>\n<p>We saw that a FFN is able to approximate complicated functions as long as they are single-valued and follow a causal relationship. However, when this setting is violated the network fails to find any reasonable approximation to the data. We can repair this by allowing the network to learn a probability distribution rather than an exact input-output relation.</p>\n<p>As take-away from all of this we should be careful when designing network architectures and think hard about the data and if it can be modelled in a causal way. This becomes especially hard in higher dimensions but also more important. It also highlights that standard loss-functions are not always suitable to a problem and the right choice can make the difference between the solution and failure of a problem.</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-fn-pressure-bulbs\">This is the reason why we see \"white\" light from energy-saving <a href=\"https://en.wikipedia.org/wiki/Mercury-vapor_lamp\">mercury-vapor lamps</a> and other vapor pressure bulbs.<a href=\"#fnref-fn-pressure-bulbs\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-fn-loss-surface\">Mathematically it is exponentially less likely with increasing dimensions that the surfaces curves upwards in every direction, unless you're close to the global maximum.<a href=\"#fnref-fn-loss-surface\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-fn-square-error-loss\">In case of a very simple likelihood function given by just a simple Normal distribution of the weights, we recover the often used <em>squared-error-loss</em>.<a href=\"#fnref-fn-square-error-loss\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","title":"Causal vs. Non-Causal Learning using Neural Networks"}},"staticQueryHashes":[],"slicesMap":{}}
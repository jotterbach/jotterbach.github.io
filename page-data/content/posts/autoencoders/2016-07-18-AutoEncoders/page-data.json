{"componentChunkName":"component---src-pages-blog-template-js","path":"/content/posts/autoencoders/2016-07-18-AutoEncoders/","result":{"pageContext":{"html":"<p>Quite in line with the last blog posts about <a href=\"https://jotterbach.github.io/2016/03/24/Principal_Component_Analysis/\">Principal Component Analysis (PCA)</a> and <a href=\"https://jotterbach.github.io/2016/05/23/TSNE/\">t-distributed Stochastic Neighbor Embedding (t-SNE)</a> I want to discuss another dimensionality reduction technique that originated in the Neural Network (NN) community, known as <a href=\"https://?\">Autoencoders</a>.</p>\n<p>The idea behind an autoencoder is conceputally quite simple and results in very powerful outcomes if applied correctly. Unfortunately, however, it suffers from the typical problem of NNs in that the outcomes are not readily interpretable<sup id=\"fnref-fn-images\"><a href=\"#fn-fn-images\" class=\"footnote-ref\">fn-images</a></sup> and somewhat harder to train. But before we jump into the autoencoder let's do some preparatory work for motivation and understanding. The <a href=\"https://github.com/jotterbach/dstk\">code</a> and and a <a href=\"https://github.com/jotterbach/Data-Exploration-and-Numerical-Experimentation/blob/master/Numerical-Experimentation/AutoEncoder_for_AdsDataset.ipynb\">notebook</a> are available on my personal github.</p>\n<h2>Lossy compression</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 340px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/099b91457d41f595b0d8f8dd3d59bf04/9f933/simple_noisy_model.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.54054054054055%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAACmElEQVR42o2TW0sbQRTHV2MrCqXQh4Lie/tssdW3QKWPfopK++IHKAiB2lc/QKGlIKV9sSV4oV5QKUZKpFJpxUQ3ZjeJ6yazt2R3zUY3u/+eyUVDoaUDf86cufzmnDMzgtBq8fjaYLnsPWPMnjQM9z9lTxaZNUl7nhpl77mmnQ8KALo5MJ8/GwlDwHXrYbUa4p/yQng1wHEvUZDOoKhGULMvaMCPXgHT6ZORSqUGxio+KdA0m1QJeL/pk8gWmR0w6SzQTvXAlJTAOhCD7MKKn/44D3FxPSpMTU31toGO46NUqoQac6DrTWmaQ4fY0LhvVmEWNJhpCaVcEXJyH5lPCzhaXg/0VBaZ5N5jYXx8/DYHyjIbdtw63xxwiG4QjFUaIIMiNwmgUXry6hbExC6O4ssobGxD/XkEg6Kt0kFWSooKY2Njd4nXoyhs2C17TaDuwiowWGIe7FRDfnkd0rv3OH47B/nrNxS3k7D2U6gcnkBXDDDDDRzLgyYScHT0yR0CdvGUzyk1o1QOWaaAQvIHMourEN/MQfqyCVVWYR5mYGZOm6UoUfRWFaxZksBx6uR7UWFiYuIWTzmVyjww9g8hfpgPxKU1ZFc2UNz7BePgGCZtMhQdGs+Aotd4TXlZNLvRbwDtSxqzGrfcxYFL8aVH2a0dqJuJS2Pnu2+ppk81rLOyx2/dpzKQb/usxF+B3Va9ZS9M04OqWtfPJv955SGqPmp1hDU/RO2CRG/tf+R5CIjDbbT9UYTXL2L31ePcTFF3p1mxPC1nlVdZEp0+zZXLlV5KkjrT9nlfltXGfJHWK4oxk0js3rsCDg0N9ZGJCNftBulmh9/b0t/muwcGBvqFzkZRR0g9LUU6/Mjs7GxfLBbr7xz7Yz1X12+Gx/q9FoyAuQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Linear model wtih noise\"\n        title=\"\"\n        src=\"/static/099b91457d41f595b0d8f8dd3d59bf04/9f933/simple_noisy_model.png\"\n        srcset=\"/static/099b91457d41f595b0d8f8dd3d59bf04/12f09/simple_noisy_model.png 148w,\n/static/099b91457d41f595b0d8f8dd3d59bf04/e4a3f/simple_noisy_model.png 295w,\n/static/099b91457d41f595b0d8f8dd3d59bf04/9f933/simple_noisy_model.png 340w\"\n        sizes=\"(max-width: 340px) 100vw, 340px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\nTo illustrate the idea let's look at a simple model where we have two dimension that follow the relationship</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>y</mi><mo>∼</mo><mi>α</mi><mo>+</mo><mi>β</mi><mi>x</mi><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">y \\sim \\alpha + \\beta x + \\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">ϵ</span></span></span></span></span></div>\n<p>with a noise term <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mi mathvariant=\"script\">N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>0.1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\epsilon \\sim \\mathcal{N}(0, 0.1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">ϵ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">0.1</span><span class=\"mclose\">)</span></span></span></span></span>, a bias <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span> and a slope <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></span> as show in the figure to the left. To truthfully represent the full data we would have to store <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">2N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> floats as we have two dimensions and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> data points. However, it is quite obvious that we would also store a lot of the noise that is actually not important for the model. If we can afford to \"loose\" some information it would be possible to store the data with just <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>+</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">N+2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span></span></span></span></span> floats -- <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> values for <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span></span>, the bias <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span> and the slope <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span></span></span></span></span>. For large <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> we would get a compression of a factor <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span></span></span></span></span> by reducing the noise. As a side benefit, we made the considerably simplified the data, as we reduced the number of dimensions from two to one! This is the general idea for lossy compression algorithms<sup id=\"fnref-fn-jpeg\"><a href=\"#fn-fn-jpeg\" class=\"footnote-ref\">fn-jpeg</a></sup> to reduce the data by removing \"noise\".  The idea of removing noise to reduce data is directly generalizable to higher dimensions, though technically significantly more challenging.</p>\n<h2>Non-linear Representations in Neural Networks</h2>\n<p>The second ingredient we need before diving into the Denoising Autoencoder (DAE) is to understand what simple NNs learn to separate features that are not linearly separable. I will steal this beautiful example from <a href=\"https://colah.github.io/posts/2015-01-Visualizing-Representations/\">Chris Olah's blog</a> which is defintely worth reading as well.\nLet's have a look at the blue and red curves in the original image below\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAv0lEQVR42pWO2wqCQBBA/f8vCHrsC4KosJCIUggLhC5aIWaQCaYurO6sl922LOwlo/MwDDNzZkbiTwAIJBhIihCKoigMwyAIbm+qvIpiAAAqSyqKwjg46nqnLA3DPFIglNI8z4sviFYtZxQ6k1Vbs1ozc6hvOWeMN8EYq2WgdDOaWrJu9tW9pjebgrIsP+QscwZK0hujrnyaL8q/ZPEGxtix7bPrpoT8cnmS4KvvixUP+VVKU/Le1wyKY8+7VGfufiqSU2DdHBUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Logit on linearly inseparable classes\"\n        title=\"\"\n        src=\"/static/e81b97abca45503bb92359bca506ff3b/fcda8/netvis-simple-NoHid.png\"\n        srcset=\"/static/e81b97abca45503bb92359bca506ff3b/12f09/netvis-simple-NoHid.png 148w,\n/static/e81b97abca45503bb92359bca506ff3b/e4a3f/netvis-simple-NoHid.png 295w,\n/static/e81b97abca45503bb92359bca506ff3b/fcda8/netvis-simple-NoHid.png 590w,\n/static/e81b97abca45503bb92359bca506ff3b/9f82e/netvis-simple-NoHid.png 820w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\nA neural network without any hidden layers and a sigmoid activation function is a fancy way of writting a <a href=\"https://?\">Logistic Regression</a> which is a linear model. The trained models learns that the best possible way to split the two curves by classifying samples within the blue and red shaded regions. However, in the center we have misclassification of blue points, whereas on the edges we misclassify red points. The linear decision boundary minimizes the misclassification rate as any other linear separator would do a worse job.</p>\n<p>The introduction of a hidden layer changes the game dramatically!\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1b831b86fc68f81485ec7cc4e0165e3a/d4c13/netvis-simple-2S.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.486486486486484%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABDUlEQVR42nWQ4WqDMBSF+/4PsD3Bfnelo26jMmfnnAitHa50DtFhtSbUVIVpNDHd1bJWCjs/LuFyvnO4GRw60bLEURQGgef7AUzPCzcbvI3SLEvzPOmJEFJV1ZEacM71lassV5JhGbYDKT9FAdOw3oezhamb0adTc950EkIwxnpwRW9l80p3rmdrzVjCSrR7caNaY9VORtPvx+dKNIc/QcQZhlcoyQt5/iG9xi9vTUcDP59q5Z3CJgqybMrZP3BdOw8KHt1Hw4mrm12rAPjrSY01Y+/6GOOT+xIGLxyEEN4RAkedTKxptskuxLhm51pQlmdxHEPEEW5VwCdR2jdB/Z4kUMsu4DRFCFHalv8Co2uHWWsi4dsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"NN on linearly inseparable classes\"\n        title=\"\"\n        src=\"/static/1b831b86fc68f81485ec7cc4e0165e3a/fcda8/netvis-simple-2S.png\"\n        srcset=\"/static/1b831b86fc68f81485ec7cc4e0165e3a/12f09/netvis-simple-2S.png 148w,\n/static/1b831b86fc68f81485ec7cc4e0165e3a/e4a3f/netvis-simple-2S.png 295w,\n/static/1b831b86fc68f81485ec7cc4e0165e3a/fcda8/netvis-simple-2S.png 590w,\n/static/1b831b86fc68f81485ec7cc4e0165e3a/d4c13/netvis-simple-2S.png 825w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>\nThe NN learns a non-linear transformation, that makes the two classes <em>linearly separable in the hidden layer</em> and hence forms a non-linear decision boundary in the original input space. To visualize this we can look at the distorsions of the underlying grid, when going from the input to the hidden layer. As a result the NN is able to perfectly speparate those classes<sup id=\"fnref-fn-overfitting\"><a href=\"#fn-fn-overfitting\" class=\"footnote-ref\">fn-overfitting</a></sup>!</p>\n<h2>The Denoising Autoencoder</h2>\n<p>Now we can tackle the DAE: We combine the concepts of lossy compression and non-linear representation. The idea is to learn a non-linear representation of the data that minimizes noise while maximizing the ability to truthfully restore the data from the compressed format. The network structure is quite simple and consist of two major components, the encoder and the decoder network.</p>\n<p><strong>Encoder:</strong> Here, we start with <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span></span> input nodes, corresponding to the dimensionality of the input data and reduce this to <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span> nodes corresponding to the desired dimensionality size of the compressed data, i.e. we learn a function</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>e</mi><mo>:</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>m</mi></msup><mo>↦</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>k</mi></msup><mo separator=\"true\">,</mo><mi>x</mi><mo>→</mo><mi>z</mi><mo>=</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>e</mi><mo>⋅</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">e: \\mathbb{R}^m \\mapsto \\mathbb{R}^k, x \\rightarrow z = \\sigma(e\\cdot x).</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">e</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7254em;vertical-align:-0.011em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7144em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">m</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">↦</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0935em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord\">.</span></span></span></span></span></div>\n<p>where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span></span> is the activation function and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">e</span></span></span></span></span> is a <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">k\\times m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span></span> matrix of weights. Even though this looks like a simple matrix multiplication, this need not be the case as we can string several hidden layers together to for the transformation <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">e</span></span></span></span></span>, thus making the represeantaion more non-linear.</p>\n<p><strong>Decoder:</strong> there are two generally used decoder types - the tied and untied decoder. In case of a tied decoder, we use the inverse transformation <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">e^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span></span>, which for a single layer is just the matrix transpose. This results in a stiffer system, but has the advantage of a having to learn fewer parameters and is hence less prone to overfitting. The untied decoder learns a complete separate representation:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>d</mi><mo>:</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>k</mi></msup><mo>↦</mo><msup><mi mathvariant=\"double-struck\">R</mi><mi>m</mi></msup><mo separator=\"true\">,</mo><mi>z</mi><mo>→</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo>⋅</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">d: \\mathbb{R}^k\\mapsto \\mathbb{R}^m, z \\rightarrow \\sigma(d\\cdot z).</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9101em;vertical-align:-0.011em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">↦</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9088em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7144em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">m</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mord\">.</span></span></span></span></span></div>\n<p>to efficiently map the compressed data to the originial input space. Here <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span></span></span></span></span> is a <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">k\\times m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7778em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span></span> matrix. This type needs much more training data as the complete network has typically twice as many parameters as the tied autoencoder network.</p>\n<p><strong>Learning objective:</strong> The reason why this system is called <em>autoencoder</em> is the objective funtion the system wants to minimize:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>L</mi><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy=\"false\">(</mo><mi>x</mi><mo>−</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo>⋅</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>e</mi><mo>⋅</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">L = \\sum_i (x-\\sigma(d\\cdot \\sigma(e\\cdot x)))^2.</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.3277em;vertical-align:-1.2777em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.05em;\"><span style=\"top:-1.8723em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span><span style=\"top:-3.05em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.2777em;\"><span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">e</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1141em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">))</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8641em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mord\">.</span></span></span></span></span></div>\n<p>It tries to optimize the decoded representation of the data with respect to itself. It should be pointed out that the loss function doesn't need to be a squared error loss, but can be chosen appropriately for the input data, e.g. log-loss for binary input data. Note that depending on the activation function the input data needs to be scaled. E.g. the sigmoid clamps the output to the interval <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-1, 1]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">−</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span></span></span></span></span> and hence the data has to be scaled to fit within this range for the autoencoder to work correctly.</p>\n<h2>What is this good for?</h2>\n<p>The first thing to note is that it an unsupervised technique, meaning it doesn't need labelled data. This in turn means that it can be used for clustering or as a feature preprocessor by learning new representations. Learning an encoder lets us reduce the dimensionality of the data while preserving a lot of information by learning important, potentially nonlinear, features. The dimensionality-reduced data can then be fed into a unsupervised / semi-supervised clustering algorithm that can now perform its magic without suffering from the <a href=\"https://jotterbach.github.io/2016/05/23/TSNE/\">Curse of Dimensionality</a>. In some cases further clustering might not even be necessary as the non-linear features already learn such structures in the compressed data.</p>\n<p>A second use case is the pretraining of a Deep Neural Network (similar to a Deep Belief Network). In this case the autoencoder learns a good weight initialization that can then be used to further train the network using supervised techniques</p>\n<p>###Example\nTo demonstrate the workings of the autoencoder, I want to create a two-dimensional visualization of the <a href=\"https://archive.ics.uci.edu/ml/datasets/Internet+Advertisements\">Internet Advertisment dataset</a>. The data contains 1560 features and only 3300 records. Hence we easily run into the curse of dimensionality as the density in the high-dimensional space is quite small. <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 531px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.5945945945946%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAAC6ElEQVR42o2Uz0/TYBjHm6AeRA7Gg/FmiHcTlYwYjBcTiJxM2MHEM/EfUA5quBg9edJEL6IJZjHMORYPJIpyIDJBwgBlw8G20vV33/5Y222w/nh83g4V1BibfPM8ffv20+/7fdsyw8PDBxmGOVHe2Iobhn1H09wbmmaP7JWu2yOCoI0oihX1P8YlSY+k6+5NQuxbsmx1MwDQgUAmuyon7QZA4IO3sxNCqwX75Lo70Gz6UU+vex5Avd6KhGMBcujYADMxEQEPzy4U0p+/ymDo7raq1vy9Qjd+tar6kmREPTqNKrpGEdq3LGsb0Gn/T4elEp9aXFNBEC2PaDao6i9pmgPVqgaSZEa9otSiijCUTucEjuPhuDvAxOPxIxQo8HIqX7Igv6F6lllvg/YAeZ78BaiDKP4G7Ovr60ZeJ8vyk8TchoVVwSPEARlvEiULdOzpOYcOcalRr2m1qFIYxoDnduC6Hl3yANPb23uKOtzakpPNRggrBdlDKLCcAbm8FLnheBM+5VhQZCtyhrlGlQJF0djvMBaLnaQONze5jOP44NpNb36Fh+TbdQSKUCxrsIzgzPtC27HuRjEQ4kawtkMHHfpth6Ojo4eow0pFTtKn4GRP1x1Ivy/Ck1fLkM1VYTpbgRdvVkASTbyOcaBTCqW5/pHh7i53FovspOO0aB4tXFJoW244M18J741lw+v334W3H82ERLVC03BRTlgz66Es6SEvkBCBvm239gEZlpVSrhtQ+147JxucWgMI9rOLVRhL5+DLugRrGyrNGQqbGpQqKqi4UfjuBs1GAJa+B1guCymaIV0yzQRrKCt2Ox+7GRDVxI0i4eq64n9c4iA1XYRnmeVg7ZsUslUjEJUGcILVT1lMV1fXsURi8hzP6+cJqccSidTQ3NzSJfykzk5NfRgcH58Y4jirJ58vXsy8Tl5t2MYFNNLz9Hli6MHjl5dT08KZuw/T/YNXrp1m/uM4QH8eqOO79eg/5nZ8B7+xUi0rvtIcAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Training loss for Autoencoder\"\n        title=\"\"\n        src=\"/static/a18f70647508c9135cf8eed33e2b8230/d4713/traing_loss_ads.png\"\n        srcset=\"/static/a18f70647508c9135cf8eed33e2b8230/12f09/traing_loss_ads.png 148w,\n/static/a18f70647508c9135cf8eed33e2b8230/e4a3f/traing_loss_ads.png 295w,\n/static/a18f70647508c9135cf8eed33e2b8230/d4713/traing_loss_ads.png 531w\"\n        sizes=\"(max-width: 531px) 100vw, 531px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>{: .text.img-right width=\"60%\"} Using a deep autoencoder to reduce the dimensionality to two dimensions we can get an intuition about how the data is distributed and what kind of features can be learned. For the plots, I implemented my own Autoencoder using <a href=\"https://www.tensorflow.org/\">TensorFlow</a><sup id=\"fnref-fn-jmetzen\"><a href=\"#fn-fn-jmetzen\" class=\"footnote-ref\">fn-JMetzen</a></sup>. The code can be found in the <a href=\"https://github.com/jotterbach/dstk\">DataScience ToolKit</a> package on my github repository. The figure showing the training losses displays the typical learing curve of a Neural Network in that it has plateaus followed by steep rapid descends in loss. The black dashed lines indicate the epochs where the learning rate was adjusted.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 523px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7610c474bb45d522806fabb43f7b24cb/3e286/first_layer_output.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.27027027027026%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACr0lEQVR42p2SX0hTURzHryFEGj6kskoReuiht72lhS6nkdrYQ2Y+SGSh5FMGy57EGfSQ4RJ7CiFNUxZZmiOhIgiEJEu2loVBbmrp/tzt7u7eu03duff++p1de1BnRD/4cg7nz/d8fr/zYxiGySguLi4IBHgLx4n3wmGhKxwW7/6HullWOMrQsNls+xRFdQOGooCiqgD/KjyvUoEWldQvw2AwFIZCgisaXQMck/iSTIXzLSMVx0myZ5GVXV9X5AjO2ZBAcJ2EQiLelSpShFarNQsX3JKUpIaKtikCprFlxIsQFxMw990P/WOzEMM5rqlUkUgcz0kaYVVVVSEufvmbIcuKwEdisLDIwsALJ4y8/AyiEN9pqNPpsv8QimKSHlDoZSqNSlMwKIDAx8E974NLHRMwiKaikIBAUFBxT+W4OAQCaFhUpDtiNBoLdiOkZlirVLoSGkx98sLlTgcMTjhTKQfTpJxpsViyqeF2Qo1SAs9SCFZ9PCSkdUzVDRfbx2HY4UpPSKOmpuZgMBid205IRev2bsaDpmEg60l4/uYb1N4YTX2KtAth6pfTEeLrEBPWYNq1DH5/FFZWI9Dz+D2UNA5AW89r4MIS1lbcSWgymfL8ft4diSQo1QaK9hWJS2vEOfeTPBidke+PTJNrdyblplsT5OSVR8Rie0WwpoTnYwRrTXtRDQSimmFvb+9erJWL59cp2YbPxyvLvzh56qOXNHWOq2XNQ+RU86BacXVIMV9/olS2jCh1bU9xf0Gddi4R7Es5yicoodbYer0+3+GYrPb5RIPd/qz+w+x8eXffmOl4rfXcica+0tILHQ3m1uGy8oYuc0l9l+l0S79Rb24/X99mLz3beLvuof1ttfeH90xr681jzGZkbCpd7MnNZQ4X5uQcyMvKOpSfz+xndo/M39Y0MnxmuTcjAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Output values of first layer\"\n        title=\"\"\n        src=\"/static/7610c474bb45d522806fabb43f7b24cb/3e286/first_layer_output.png\"\n        srcset=\"/static/7610c474bb45d522806fabb43f7b24cb/12f09/first_layer_output.png 148w,\n/static/7610c474bb45d522806fabb43f7b24cb/e4a3f/first_layer_output.png 295w,\n/static/7610c474bb45d522806fabb43f7b24cb/3e286/first_layer_output.png 523w\"\n        sizes=\"(max-width: 523px) 100vw, 523px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>To ensure that the autoencoder is not saturated, i.e. that the hidden layers by default output <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1</span></span></span></span></span> or <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"></span><span class=\"mord\">−</span><span class=\"mord\">1</span></span></span></span></span> we look a the mean value of the outputs of the first layer (and strictly speaking we should also look at the other layers).</p>\n<p>To see what feature distribution the network learned, we can look at a scatter plot of the encoded representation. Note that the AE distributed the features nicely across the range <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">]</mo><mo>×</mo><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-1, 1] \\times [-1,1]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">−</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">−</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">]</span></span></span></span></span> making maximum use of the available space; another indication that the individual layers are not saturated.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 522px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/55a4d12aafa4699538801d1a8bf6a038/29492/encoded_representation.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 70.94594594594594%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAADbUlEQVR42n1TTWtcVRi+KVjwA5UguhNcKCGLZJGdLhI3LYiQheAPKLjQAV2KSAlqwRAFrVJIbbMoQ5pa08nQOGmSMcnYZJp0Zsxk0plJJ5nM552Z3nPP/f6Yyb03eTw3TdWF+sLDeT84z3nPOc/LcY/taZ4n5yRJv0iINkqIPibL+hil/w+ZYb9ujTWbytfVSuOSIEhvcgC6fMYD5zDEfBweHnmeBzgHvg/4vus+9v31SezXXZYjVRtUNB2bxW3LCZw0yJ0mghQSNWA+qncqvOVatukW91S3xpuuSHU3n1ddQdDdVktnseEKLdUl1UeuQYhbzxVtUuChtr0P/+pQtp0QWd/BUjDjzV/bxvoSj+lpis27NWiKgoWoikRSQ6moQLdsFFaymJvcRfr+IyRuphwl24Dc9gLcwMDAC4zvGaKaYfqgBmG34i2NTGLmm1UIyT/Q3HyIalmBRFTs74gg6gGKmQbmz/+C3+Z4xBYboNRw/OsLghrg+vr6XvM71AwjlE4YIIrt3U8quPpFCmK5jlKzg62sCr4igWQLSP6axX5RRfp6DBNvfY741WVUarqTiytoe+0A19vb+yrje5YoRlguUTyM73qlZAkrkQYEzUV8PIrkxRCo2oZADKxGeSwuUCQyHUy/MoTY+x9DPIDj2YAoGgGup6fnDUbYJUrmLU12kbqy6E1+MoPIz2UIDYpCoozcwhYIL4J9CK6NpvBdIIbIVAnjr3+A8PlpFGbXnFbFgWFZAa6/v/9Fv8MWT8NMEahl9r3L31fw7YUqJs6MYmP0Ou6Fcsg9kDH11e/Iruwgf7eARiqLH4avYDm4hUa+4tRXizCAADcyMnLKf0Mit0NmqY69HdEL3tAx9WkEkc+mEJtYx+xPGdz4MYvL52aQjvOobpVxZyKN8S/TmL1wB7V7WacZ3Ybq/U14WiBa2JJMdk3Jo7IJU1OhWB2IhgtR7UDTDKi2A0XRISkm+JYFVdGgEIpmjTq+0I/f8IkOJdG85bAJEKnlMeWzonUksk+gPqjJcBwficc1E7J0kpNsSJLl+FMmy9ZHx2PS3d39/O3bc4ONBj3DThlieHt5ee29vb3aWUVpD+m6M7i2tjmcSuXf9Wt+bnu78M7GxuawLNuD/h6mwbPB4M1e7h/Wxf27PcXwEsPLJ3iO+2879Sc1MDORWzTVUwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Encoder representation\"\n        title=\"\"\n        src=\"/static/55a4d12aafa4699538801d1a8bf6a038/29492/encoded_representation.png\"\n        srcset=\"/static/55a4d12aafa4699538801d1a8bf6a038/12f09/encoded_representation.png 148w,\n/static/55a4d12aafa4699538801d1a8bf6a038/e4a3f/encoded_representation.png 295w,\n/static/55a4d12aafa4699538801d1a8bf6a038/29492/encoded_representation.png 522w\"\n        sizes=\"(max-width: 522px) 100vw, 522px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>The AE learns to distinguish some kinds of advertisment from non-advertisments quite well, so we could train a good classifier on those. However, we also see a big cluster, where ads and non-ads overlap significantly. We can conclude that in this region the spaming ads are very successful in hiding between the non-ads and we might have to do more feature engineering to find good splitting features.</p>\n<p>###Final notes for training an AE\nTo summarize, I want to provide a list of things that you can try for successfully training an AutoEncoder:</p>\n<ul>\n<li>Use the <a href=\"https://en.wikipedia.org/wiki/Activation_function\">softsign</a> function as activation. It is not as steep as the sigmoid and does not saturate as easily</li>\n<li>To choose the number and size of layers make reasonable steps between the input and output layers. Don't make the model to complex/big as you need more data to train it. Also think of using a tied encoder if the available data is small.</li>\n<li>Normalize and center your data to be uniform around <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">0</span></span></span></span></span></li>\n<li>Adjust the learning rates during training</li>\n<li>Monitor your layers to avoid saturation</li>\n</ul>\n<p>I hope this was helpful. As alwyas a <a href=\"https://github.com/jotterbach/Data-Exploration-and-Numerical-Experimentation/blob/master/Numerical-Experimentation/AutoEncoder_for_AdsDataset.ipynb\">IPython Notebook</a> is available on my personal github account. Enjoy playing with AutoEncoders!</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-fn-images\">There is one recent exception for image classification. It turns out that due to convolutioon and pooling increasing number of layers learn more and more complex features, that tend to <a href=\"https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\">represent structures of the input images</a>. However, these cases are for foudn in supervised learning as opposed to autoencoders.<a href=\"#fnref-fn-images\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-fn-jpeg\">One such compression algorithm is jpeg. In a nutshell: To compress the data a two-dimensional Fourier transform is applied to the image. Afterwards the high-frequency components are removed, the inverse transformation performed and the resulting picture is stored. The result is a \"washing out\" of sharp edges resulting in a loss of contrast and a blurrier image, but reduced size.<a href=\"#fnref-fn-jpeg\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-fn-overfitting\">In practice this perfect separation might not be desirable as it indicates strong overfitting to the training data and makes generalization to new data questionable.<a href=\"#fnref-fn-overfitting\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-fn-jmetzen\">I also relied heavily on ideas presented in the very nice blog post of Jan Hendrik Metzen on <a href=\"https://jmetzen.github.io/2015-11-27/vae.html\">Variational Auto Encoders</a><a href=\"#fnref-fn-jmetzen\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","title":"Unsupervised Representation Learning a.k.a. Autoencoders"}},"staticQueryHashes":[],"slicesMap":{}}
{"componentChunkName":"component---src-pages-blog-template-js","path":"/content/posts/pca/2016-03-24-Principal_Component_Analysis/","result":{"pageContext":{"html":"<p>A typical approach in Data Science is what I call <em>featurization of the Universe</em>. What I mean by that is that we extract and engineer all the features possible for a given problem. To give an example: In a timeseries problem, one could use cumulative sums, moving averages with variable window sizes, discrete state changes, average differences, etc. as features, which quickly becomes very large. Alternatively, data source can be so rich that already the raw extraction produces a vast amount of data, e.g. dummification of the U.S. states leads already to 50 variables <sup id=\"fnref-fn-independence\"><a href=\"#fn-fn-independence\" class=\"footnote-ref\">fn-independence</a></sup>.</p>\n<p>In this case <a href=\"https://en.wikipedia.org/wiki/Exploratory_data_analysis\"><em>exploratory data analysis</em> (EDA)</a> is challenging and we need to resort to alternative methods of visualizing and exploring the feature space.</p>\n<p>One approach is to reduce the dimensionality of the feature space and poke around in this reduced feature space. A basic technique well-suited for this problem is the <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\"><em>Principal Component Analysis</em></a> which tries to find the directions of most variation in your data set. Let's look at a simple 2D plot to understand what that means</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 570px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e1e4ef4244aa8b2dcb6762fcb1543bd4/432e7/pca_2D.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 129.7297297297297%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAaCAYAAAC3g3x9AAAACXBIWXMAAAsSAAALEgHS3X78AAAEMUlEQVR42o1V3U9TZxzGCxOuAGFqRMSoy/YvcL1d7crNS0mWbLsxS3axJfu42paQzGQfoejItLKMrQnbCLJVK4FMs9nO2ZaiCNgCUmo/oOejPef0tJRKsc9+v7c9trSd2UmevO85fd/nPM/v+Z23LS109ff390iS/oGq5j9R1dzH/xeybFSgfyhJxmfpdOE15msxDOPAzk5JL5WAJ09Q2tkBnodCoSTGaDSJ5eV1hMOJomnu4OlTfC8IfT5fl6JkU4piQpYzRUXJ7NIoUDu3QG52ZXr+8GF4d25uaXd+PlQgUiQSqSuCcGBg4BQtzGjaFoigpKommJzB83qk1GxlnkUqleOxWCyCxJhlhbqODtqsp9NbrFAQ0o8CZdXVF1gvqb1nV1SqKmEwGOyiG6OWsLrYhCRVX8Cod9BAaLNd7qW6ZC3LVM9nm+NxE8lklYQ2N5SA9gjLNG9umYkikQwlaMDr1eD3awiFdCo6bVZzDfYbFAYCgRcsy6yQCROJDJEYmJ5OwW6XMDgoYfxXGYvzSQoiC6vOTQmHh4ePUYOalmXewG9nq16vjkvfJfHmWxLeeHUaFz6/C18gj8i6UUlZKN1juT0aNQ7Up8yEVhhzc2kMXkzj3bNunDvtwqUrGjyetCgLr5GkGoWtra29TufNw83axhoj6zr+vmtg3B7GyDkXLnyr4Pr1lCgJu9hjmQiP22y23vrGlmSdAjArljLYlLcQ//MelqYWcNujY3ZWEworPVpt7La2thd9vmAX1UMTocjm3j4kMlkyIGvb0G55oCytYUPJIxHP0DdsVMqSKfK3bYWy3+12H6SHlHKelBklgui3TUlDkqCwffrEtKlbUCJJqPRiJorHLYV1KY+Ojh6lwlLKeVZU2pTSWIvFEVhdxOzKA4RiUWw8ikCfJkLRWuVSWN9zNeVsfWOXFW4Q4Wo0hr8W/4HD8zMu3v4Bk5PDWHX+DkkvIKVkBZHMZWnWhzMzM4esUFQRSpaJEXocFoTvXf0I/Z++gq++eR/utfuIxBNIypo4dco9W/fpjY2NddOJa9YfX2w9GAnjpzu/4G37O3j9/Bl8PWXDzQU3VqkMotaq8YxQVp9zfFkpxzYV3AkGMOG/hgHneXxx7Uv85r+B+4+CiG+q5S6ot+xwXD1Cm41ahbVge48TSSysL+PGvT/gXvJiORqh5+VeVdWmljOZZgpF0UkFNzpbZFUc2npiA/FkWSGnvEchW6YgdG4b+lGEUgvFao+akUmZjP8CGtpmZGSkm04bhWtB2CYUGLRwm0pQoIV5C0QinjEkuTwScqZZZHeXBWFfX1+n3f7jSwAOOp0zJyYnp05lszg0MeE6yfOVldhRh2P8ZZ6HQtFuHv3+xWO05rDL5Trp9T7oUZTckaGhoZ6WyrWvgmbX/o6OjuN8KrW3t5/o7Oxsa/nva9+/Kg2JGVBzv3EAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"PCA in 2 dimensions\"\n        title=\"\"\n        src=\"/static/e1e4ef4244aa8b2dcb6762fcb1543bd4/432e7/pca_2D.png\"\n        srcset=\"/static/e1e4ef4244aa8b2dcb6762fcb1543bd4/12f09/pca_2D.png 148w,\n/static/e1e4ef4244aa8b2dcb6762fcb1543bd4/e4a3f/pca_2D.png 295w,\n/static/e1e4ef4244aa8b2dcb6762fcb1543bd4/432e7/pca_2D.png 570w\"\n        sizes=\"(max-width: 570px) 100vw, 570px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Looking a the 'original' data points (blue dots) it is quite obvious that dimensions 1 and 2 are not independent and that neither of them is ideally suited for describing the data. However, looking at the two indicated vectors in the data it seems we can find a transformation that will result in completely independent dimension -- or put another way it will de-correlate the features in the new coordinate system. As an added benefit, the new dimensions align with the directions of maximal variation, and hence we can assign a sense of <em>importance</em> with that dimension.</p>\n<p><strong>Why is this useful?</strong> Imagine that the dimensionality of the feature set is larger than just two or three. Using a PCA we can now identify what are the most important dimensions and just keep a few of them to explain most of the variance we see in out data. Hence we can drastically reduce the dimensionality of the data and make EDA feasible again. Moreover, it will also enable us to identify what the most important variables in the <em>original</em> feature space are, that contribute most to the most important PCs. Intuitively, one can imagine, that a dimension that has not much variability cannot explain much of the happenings and thus is not as important as more variable dimensions.</p>\n<p><strong>How do we get there?</strong> Let's talk some math (feel free to skip if you don't want to). Technically, performing a PCA is just finding the <a href=\"https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\">eigenvalues</a> <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>λ</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9805em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span></span> and <a href=\"https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\">-vectors</a> <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"double-struck\">e</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{e}_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span></span> of the data's correlation matrix <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Σ</mi><mo>=</mo><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">\\Sigma = Y^T Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Σ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span>, where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>−</mo><msub><mi>μ</mi><mi>X</mi></msub></mrow><annotation encoding=\"application/x-tex\">Y = X-\\mu_X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span> is an <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N\\times M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span> matrix of data points: each row is one of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span> samples with each column representing one of the <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span> features. <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>μ</mi><mi>X</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mu_X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> is the empirical mean value of the data, and we will come back later to why we need to substract it.</p>\n<p>Since <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Σ</mi></mrow><annotation encoding=\"application/x-tex\">\\Sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Σ</span></span></span></span></span> is positive-semidefinite the matrix of eigenvectors can be used to <a href=\"http://s-mat-pcs.oulu.fi/~mpa/matreng/ematr4_2.htm\">bring the correlation matrix to its diagonal form</a></p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"normal\">Λ</mi><mtext> </mtext><mo>=</mo><mtext> </mtext><mi>V</mi><mi mathvariant=\"normal\">Σ</mi><msup><mi>V</mi><mi>T</mi></msup><mtext> </mtext><mo>=</mo><mtext> </mtext><mtext>diag</mtext><mo stretchy=\"false\">(</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>λ</mi><mi>M</mi></msub><mo stretchy=\"false\">)</mo><mspace linebreak=\"newline\"></mspace><mo>=</mo><mtext> </mtext><mi>V</mi><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi><msup><mi>V</mi><mi>T</mi></msup><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">\\Lambda \\,  = \\, V \\Sigma V^T \\, = \\, \\text{diag}(\\lambda_1, \\dots, \\lambda_M) \\\\\n = \\, V Y^T Y V^T.</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Λ</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\">Σ</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">diag</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">M</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord\">.</span></span></span></span></span></div>\n<p>Since <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Λ</mi></mrow><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Λ</span></span></span></span></span> is a diagonal matrix we can now associate certain eigen-vectors with the direction of most variation (the ones with the largets eigenvalues). However, calculating the whole matrix <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y^TY</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span> might be very costly (not to speak of its full diagonalization!) and we might not be interested in all the eigenvalues and -vectors, but merely want to keep the largest ones.</p>\n<p>To this end, we have to use a generalization of the eigen-decomposition known as the <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\"><em>Singular Value Decomposition</em> (SVD)</a>. Any positve semi-definite <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">m\\times n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span></span> matrix <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span></span> can be represented as a product of three special matrices</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>A</mi><mtext>  </mtext><mo>=</mo><mtext>  </mtext><mi>U</mi><mi>S</mi><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">A \\; = \\; U S V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span></div>\n<p>where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">U</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span> are of dimension <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m\\times m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">m \\times n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">m</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n \\times n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span></span>, respectively. Moreover, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span></span></span></span></span> is a rectangular-diagonal matrix with positive-semidefinite entries - the <em>singular values</em>. The columns of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>U</mi></mrow><annotation encoding=\"application/x-tex\">U</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span> are orthogonal and normalized (or orthonormal) and hence fullfill the relation that <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>U</mi><mi>T</mi></msup><mi>U</mi><mo>=</mo><msub><mn mathvariant=\"double-struck\">1</mn><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">U^TU = \\mathbb{1}_{m\\times m}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2583em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">m</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\">m</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>V</mi><mi>T</mi></msup><mi>V</mi><mo>=</mo><msub><mn mathvariant=\"double-struck\">1</mn><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">V^TV = \\mathbb{1}_{n\\times n}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8528em;vertical-align:-0.2083em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2583em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">n</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\">n</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2083em;\"><span></span></span></span></span></span></span></span></span></span></span> <sup id=\"fnref-fn-orthonormality\"><a href=\"#fn-fn-orthonormality\" class=\"footnote-ref\">fn-orthonormality</a></sup>. So let <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mo>=</mo><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">A=Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span></span></span></span></span> then we can easily calculate</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi><mtext> </mtext><mo>=</mo><mtext> </mtext><msup><mi>V</mi><mi>T</mi></msup><msup><mi>S</mi><mi>T</mi></msup><msup><mi>U</mi><mi>T</mi></msup><mi>U</mi><mi>S</mi><mi>V</mi><mspace linebreak=\"newline\"></mspace><mo>=</mo><mtext> </mtext><msup><mi>V</mi><mi>T</mi></msup><msup><mi>S</mi><mi>T</mi></msup><mi>S</mi><mi>V</mi><mspace linebreak=\"newline\"></mspace><mo>=</mo><msup><mi>V</mi><mi>T</mi></msup><mi mathvariant=\"normal\">Λ</mi><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">Y^T Y \\,  = \\, V^T S^T U^T U S V \\\\\n = \\, V^T S^TS V \\\\\n = V^T \\Lambda V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8913em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span><span class=\"mspace newline\"></span><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord\">Λ</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span></div>\n<p>Comparing this expression with the one above we are finally able to define the <em>principal components</em> as</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><mo>=</mo><mi>Y</mi><msup><mi>V</mi><mi>T</mi></msup><mo>=</mo><mi>U</mi><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">P = Y V^T = U S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8913em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span></span></span></span></span></div>\n<p>There are a couple of <strong>interesting and useful</strong> relationships in here:</p>\n<ul>\n<li>\n<p>Since <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Λ</mi></mrow><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Λ</span></span></span></span></span> contains the eigenvalues of the correlation matrix, its entries corresponds to the variances of the data in a given direction. However, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Λ</mi><mo>=</mo><msup><mi>S</mi><mi>T</mi></msup><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">\\Lambda= S^TS</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Λ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span></span></span></span></span> and <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Λ</mi></mrow><annotation encoding=\"application/x-tex\">\\Lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">Λ</span></span></span></span></span>, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S</span></span></span></span></span> are diagonal, so that <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><msqrt><msub><mi>λ</mi><mi>j</mi></msub></msqrt></mrow><annotation encoding=\"application/x-tex\">s_j = \\sqrt{\\lambda_j}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.24em;vertical-align:-0.3508em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8892em;\"><span class=\"svg-align\" style=\"top:-3.2em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"mord\" style=\"padding-left:1em;\"><span class=\"mord\"><span class=\"mord mathnormal\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.8492em;\"><span class=\"pstrut\" style=\"height:3.2em;\"></span><span class=\"hide-tail\" style=\"min-width:1.02em;height:1.28em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"1.28em\" viewBox=\"0 0 400000 1296\" preserveAspectRatio=\"xMinYMin slice\"><path d=\"M263,681c0.7,0,18,39.7,52,119\nc34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120\nc340,-704.7,510.7,-1060.3,512,-1067\nl0 -0\nc4.7,-7.3,11,-11,19,-11\nH40000v40H1012.3\ns-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232\nc-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1\ns-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26\nc-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z\nM1001 80h400000v40h-400000z\"></path></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3508em;\"><span></span></span></span></span></span></span></span></span></span> is the standard deviation of the data along a certain dimension</p>\n</li>\n<li>\n<p>The principal component <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>P</mi><mi>j</mi></msub><mo>=</mo><msub><mi>s</mi><mi>j</mi></msub><msub><mi>U</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">P_j = s_j U_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">U</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span></span> is then just the left-singular vector scaled by the standard-deviation of the data in the corresponding direction.</p>\n</li>\n<li>\n<p>If we only need the first <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span> principal components it suffices to multiply the data with just the first <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>q</mi></mrow><annotation encoding=\"application/x-tex\">q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q</span></span></span></span></span> rows of the right-singular vectors <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span></p>\n</li>\n</ul>\n<p><strong>How does it look like in practice? Or: why scaling the data is important!</strong></p>\n<p><em>Dimensionality Reduction</em></p>\n<p>The main use of PCA is to reduce the size of the feature space while retaining as much of the information as possible. A way too see how much information we retain is to look at the <em>explained variance ratio</em> of the principal components. If we define the full variance of a data set as <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi><mo>=</mo><msub><mo>∑</mo><mi>j</mi></msub><msub><mi>λ</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\sigma = \\sum_j \\lambda_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.1858em;vertical-align:-0.4358em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.162em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4358em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span></span> then the explained variance ratio of component <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span> is defined as <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>r</mi><mi>j</mi></msub><mo>=</mo><mfrac><msub><mi>λ</mi><mi>j</mi></msub><mi>σ</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">r_j = \\frac{\\lambda_j}{\\sigma}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7167em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.3384em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9934em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">σ</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.5073em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3281em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2819em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>. To demonstrate this we use a copy of the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset easily <a href=\"https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/base.py#L303\">accessible through scikit-learn</a>. Fitting the PCA can be done in a few lines</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> sklearn<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">as</span> ds\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>decomposition <span class=\"token keyword\">import</span> PCA\n\ndata <span class=\"token operator\">=</span> ds<span class=\"token punctuation\">.</span>load_breast_cancer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'data'</span><span class=\"token punctuation\">]</span>\npca_trafo <span class=\"token operator\">=</span> PCA<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>semilogy<span class=\"token punctuation\">(</span>pca_trafo<span class=\"token punctuation\">.</span>explained_variance_ratio<span class=\"token punctuation\">,</span> <span class=\"token string\">'--o'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Looking at the output it seems that with a single component we can explain <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>98.2</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">98.2\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">98.2%</span></span></span></span></span> of the variance!\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/85d0a56bab68f7a1ea2f47223f9929b6/b5bda/explained_variance_ratio.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAABzElEQVR42p2TXS/DUBjHS7zES1wKty58A0QicUHwGTCfgUh8BXc+AhHXPgRBsoSZMYRstNZtXXvWl/VsTNvH83TrVuIldZJ/ntPT9nd+fU4qCILQs3twOKHr1U1Nq6xjNv6RdcbsLcaMReQJHcnrxzXXA6jVwMXA62u01GqeA+DP9wjYeXmZXi0oHBTFfMe4UVMqWfVK5R2w7hKwK319H2P6G8h53UV9ugGqWokSp1r1qPqG/Tc3Dyucu/AkMVfK6VBmdlToZ2Aqdb9q2w5CTPf5pQwvsgEMTclWVa3IwF4ypB74/UBALm9A5plBsWhC0II/4tAXtgzv7jLLDUPLpR3JTi6YIDZtS6WGbbu/1q+GA8lkOhYG0kua2gBQT58kBKN1UWmAmGa3YIFhGNiTSNyuhYHBg4EtzSVZh6zIfGtKoWjhBmZg7lR5G9jdPpQ28GvwT/DNCJLHdlAraAMJK/bc4WjIGP8W6IVAXui6NUewh1YYGxS0F3NlRyvX6SD3CTi4vb0zls3mZw2jPnlxcTUnivIM5zB5cnK+QDFNPnV8fLZ0dBRfDOa0js9MJBKp+fgprden48nMuNAcHc18HX2YEcwoZjhUh4QfxgeVGXszhC04GgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Cancer data set PCA explained variance ratio\"\n        title=\"\"\n        src=\"/static/85d0a56bab68f7a1ea2f47223f9929b6/fcda8/explained_variance_ratio.png\"\n        srcset=\"/static/85d0a56bab68f7a1ea2f47223f9929b6/12f09/explained_variance_ratio.png 148w,\n/static/85d0a56bab68f7a1ea2f47223f9929b6/e4a3f/explained_variance_ratio.png 295w,\n/static/85d0a56bab68f7a1ea2f47223f9929b6/fcda8/explained_variance_ratio.png 590w,\n/static/85d0a56bab68f7a1ea2f47223f9929b6/b5bda/explained_variance_ratio.png 643w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>{: .text.img-right width=\"60%\"}\nThis sounds to good to be true, and unfortunately it is. The interpretation of the singular values of the SVD underlying the PCA says that the plotted values correspond to the variance of the data along their principal components. However, that also means that original dimensions that have intrinsically a much larger variance will have more impact on the principal component. This is well confirmed if we look at the variance ratios of the data along the dimension of the original features.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d81703e780c99649f0133efc4867af6f/b5bda/original_feature_variance.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.24324324324324%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACV0lEQVR42o1SzW7TQBB2QCBR9cAVCU7wCNAKCYkDhfAMNPAMjZB4iDxEQTxBJA4cegqqkEjbpJRErSo5zY/jn8Rre2u7JI7tYWYcl5QgwUqfdv155ttvZlZRFOVGtVp9KOXkrWnKLdv2y/8D1/XLZz27bI4knbeECN4J4RUVWqo6KAEu4UbJxUUKkwn8E9MpgCtjCDk+jSkf+fekd63ZPClJOYX+wJmZlkysP2CY3hJH6PZFglUlo9F55PszwH2bBAvt9mnJcVmQfsIixuNz6PQEWBaeLzkfLNz7mgMoSFwchgnx7LDQPDop2c4EhrqbUOJVQR+0oceJJJ7zfc0FAznDWBa8+XW/tSnQIZc2FyHYdgCmlSUTbxOH0FFkgJcQZ5hcRUy9zwVvtVqnm3TD0PCS0YKgsH1MdNlhLki8uVA+ucSpX3V4fKy+CoI4K5kSUYiC8QLQdCzXkpyYl0xnHAqDzkPdQ8EURN5DckiCNGEqBwNYlPbMkeSh4Fv7/T2vhGJwaDE+ObosE2w0fryhsZvYQ7Vr47RdOOuLS1fcfISmu1kPTY8vIHQHDg/F8WagdsUHErx+eNh+TT3ApISa7TghN11jpwGXSd89LevnvG8wyvs59uPJT6DhfWTBvIeYmGBgyu/MYmcpCqYkiE6IT3voCIWZz14CD4qnLESYTblSqdzvdPSnnhetHRx8f6aqwydSRut7e43n9XpjQ8pwfWfny8ta7VsxCmHt0+dacXd3/0UYwqN6/WiDeIx/bFneA2W+CnP8ba0i7q6sKHdwv4e4vZCztH4BZp15FncvCEoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Cancer data set feature variance ratio\"\n        title=\"\"\n        src=\"/static/d81703e780c99649f0133efc4867af6f/fcda8/original_feature_variance.png\"\n        srcset=\"/static/d81703e780c99649f0133efc4867af6f/12f09/original_feature_variance.png 148w,\n/static/d81703e780c99649f0133efc4867af6f/e4a3f/original_feature_variance.png 295w,\n/static/d81703e780c99649f0133efc4867af6f/fcda8/original_feature_variance.png 590w,\n/static/d81703e780c99649f0133efc4867af6f/b5bda/original_feature_variance.png 643w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span>{: .text.img-left width=\"60%\"}\nAs suspected two features along add up to more than <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>90</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">90\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">90%</span></span></span></span></span> of the full variance of the data and hence dominate the explained variance ratio of the PCA. Also note that there are 11 orders of magnitude of difference here (the y-axis is logarithmic) so that some dimensions seem to have almost no variance at all.</p>\n<p>To prevent those few dimensions from dominanting the PCA it is highly suggested to <a href=\"https://en.wikipedia.org/wiki/Standard_score\">normalize your data using z-scaling</a>. This can again easily be done in sklearn</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> sklearn<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">as</span> ds\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>decomposition <span class=\"token keyword\">import</span> PCA\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>preprocessing <span class=\"token keyword\">import</span> StandardScaler\n\nz_scaler <span class=\"token operator\">=</span> StandardScaler<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ndata <span class=\"token operator\">=</span> ds<span class=\"token punctuation\">.</span>load_breast_cancer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'data'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n\nz_data <span class=\"token operator\">=</span> z_scaler<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\npca_trafo <span class=\"token operator\">=</span> PCA<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>z_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\nplt<span class=\"token punctuation\">.</span>semilogy<span class=\"token punctuation\">(</span>pca_trafo<span class=\"token punctuation\">.</span>explained_variance_ratio_<span class=\"token punctuation\">,</span> <span class=\"token string\">'--o'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\nplt<span class=\"token punctuation\">.</span>semilogy<span class=\"token punctuation\">(</span>pca_trafo<span class=\"token punctuation\">.</span>explained_variance_ratio_<span class=\"token punctuation\">.</span>cumsum<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'--o'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>Looking at the explained variance ratio and its cumulative version, the variances slope of much less quickly, though still almost linear in logarithmic scale. This means that we have a much better handle on the variation within the data set and reading of from the cumulative explained variance it seems that with keeping only 5 principal components we can explain <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∼</mo><mn>80</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">\\sim 80\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.3669em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">80%</span></span></span></span></span> of the full variance.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2ed84bcc0630499a48d6c8ad8f4fb1fa/a6d36/explained_variance_ratio_scaled.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.56756756756756%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACaUlEQVR42oVRy27TQBQ12xa2qGLLhh9ArYToqq0QSHwIpFBvAoiPQV0gsemiUqkqBYQqZREladO0atI6iR3Hj8TxOHb8ysOOL3cmTUBQwkhH58ydmXMfw2WL8lPbDtOEBDuSrvCSrvNZIccXGiXkAp8TTlCX+VzjhC9JFaavZIkvS1e8oMiUd0RNSx+Uv69zL7l7nEHiFOAirhvpjoXsg+MPwQ9j8IKIgWmEH8RMB+Hkd47oe9cfv+foasiEv9baUGt2xp22OyGmz2Ca3qTbdSddw52YXYrpnupZnO1NbzQYAJimn2aGNVV97YVj6HTcqN4kICs9UHUb8DJeQiB3FyMaDqmh+44Zluv1nQEG8CAmxAO97YDU6kETjRtNiyWgcQqshiVZaNg2vNRoRA370awq+phqw3ChpdogytTYBlVzWIzdY3e9vw0VrfeWBgyjH9/W0qwiTXdA0abmTexAQtawGzz/o8K2kxpNW44WzWrarjdPQkcjT6uPaIdY+b8q7P/vE+amFvGpceR6CVimPzXEsrdpBpxbZNsD6PVCsKwAOWBM8BEmm+O2X7b7ExyHMzPsv/FDAFnuxldVEQShBbVaC1lmLIoaOM7wJpE/b30GYnqsZUnufZjNcNvyQqg31LhYOE/y+TKcnVWhgPr09DKhuloVk0pFTOp1lVadoFHyy9RlhrJOPjLDT5+PHl2K4rrSJquZzI+NUuniyfW1srq/f/Ds6OjbJtW7u1+e7+193RQEaS2TOX6Rz59vBAE8zmaLW8fHua1xAGuHF4cPuVfcXep5h7t9LSNWlpaWVpDvIx7c8DK3YP0EvftxrE/0K1QAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Explained variance ratio of scaled data set\"\n        title=\"\"\n        src=\"/static/2ed84bcc0630499a48d6c8ad8f4fb1fa/fcda8/explained_variance_ratio_scaled.png\"\n        srcset=\"/static/2ed84bcc0630499a48d6c8ad8f4fb1fa/12f09/explained_variance_ratio_scaled.png 148w,\n/static/2ed84bcc0630499a48d6c8ad8f4fb1fa/e4a3f/explained_variance_ratio_scaled.png 295w,\n/static/2ed84bcc0630499a48d6c8ad8f4fb1fa/fcda8/explained_variance_ratio_scaled.png 590w,\n/static/2ed84bcc0630499a48d6c8ad8f4fb1fa/a6d36/explained_variance_ratio_scaled.png 650w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>With this we reduced the size of the input space by a factor of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">6</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">6</span></span></span></span></span> while compromising only <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>20</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8056em;vertical-align:-0.0556em;\"></span><span class=\"mord\">20%</span></span></span></span></span> of potential information!</p>\n<p><em>Feature importance</em></p>\n<p>Another common use in for PCA is to identify features that are most contributing to a the principal components. Or asked another way: If my principal component vector would be a unit vector, how would the original data point look like? This is especially important when interpretability of your analysis has to be kept high, by either knowing how the PCs are constructed or by identifying and keeping only features that have big influences.</p>\n<p>We can answer this with sklearn's PCA inversion applied to the identity matrix</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> sklearn<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">as</span> ds\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>decomposition <span class=\"token keyword\">import</span> PCA\n\npca_trafo <span class=\"token operator\">=</span> PCA<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ndata <span class=\"token operator\">=</span> ds<span class=\"token punctuation\">.</span>load_breast_cancer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">'data'</span><span class=\"token punctuation\">]</span>\npca_data <span class=\"token operator\">=</span> pca_trafo<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\n\nsns<span class=\"token punctuation\">.</span>heatmap<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>pca_trafo<span class=\"token punctuation\">.</span>inverse_transform<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>eye<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> cmap<span class=\"token operator\">=</span><span class=\"token string\">\"hot\"</span><span class=\"token punctuation\">,</span> cbar<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Looking at the resulting heatmap (notice the logarithmic scale in the plotting part)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ff88e4334e18ab03acbf3583cd2d5861/3075e/pca_inversion.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAAE/ElEQVR42n2UbVDUVRSH7+7Csm+gri+IpSZFETCoobC77C74gqGwomKKOaBSQx8wJ52afM0PmjPGgIqWL+ViY2o5mGBa4egg2GqODWompYKOiKwIgYKK7N57f50/K32r/8ff7nnmOefcexmj71pFUBIeMI8QJZ52i91zizFPg1rtadFTlsE88ozRs9Wi9eyj/FOm8mxMZB7+hHK+19Nrc3q4gZ19Gqau8+rYLoXHfj2uzUAXkxLb0W1NhZcx3NGo0aZnkFkMOGvC55YQHKS8iKmwycLAfZTzryGTnRAmxjFQjXYS6ANePaaZhjYmBIpFm9UuGhgTNzVqQYaCZzKBGpMotYQIMhRkKDZOpPwx5dwtntmcghuZTxLQq2dnA4aHybCVDGUJ2q0OEBAEVP4AMZNMakzYRoZuyjeS4YaJlD+hXJTBZ3OCWu4z9PYbXizXzqAZ9gE7ngNvKEAdFaZQ4UkTiqwh2E35OgKu7QfyMvipZT6QgGY17vcDLx/RpBMQUhbjIQFbqLA5KACUM6jwtBG7bDrsp3wTATcnqp4D3fA7UsEHEHCQBvcNfUBHxLFdxvlop5AMvckOXFcxXNEH475RBTmXoGeM2JOsxzmmxk6mQWkiZT0Bwx4nLSWKaoer0aXtA06IPOnWz1GWogAvk2EtmVRr1bhJhiBDfjIIO/Li0TQmFhUvxWDv4ljwR/Tb0514+E4hnuVEcyRF41xcTABYtUc/l4Ac2II/3kjE1aSJ6JnuxI3Rg+Ff9ipkJUPt0XW4XlqGGkc67pSvAmQI8HgHunIL8WTyWI4YHS5EhinA8aMrvgjLoRkSsBSNp37B7arjaP94NbUdBJ4fTls24m7lGvTkzEPDkkKcftMMUMsCh/HIXY7GUXr+zJWMW/0tV+019RlKAmLrOjwyM1wboYdvgBbSETjYPyRpcYHmd2veElTPHgneQTkOAOuX4lJkKO/JzgQP7gMmRX2/w7hAWYpybB7YU+ClpZx5fzG8qTbIVCqkpTRtzkFD4Uqcn5OLizRDoSwF38C/qgD+uBFcFn+CewHgcv2/QF6Czmk2tMyKw+8xsWhOHgu4aKOnQnDnshu3T9fhav4y/LXUCvg0BPwOHfvK0TWMxhX/IrxT4hQg1JUKUFmK3ILmNCcupSagNToSD8a9DjmLjk61Cc12NeqzslAfEYV72aMgelV0U9yQ1hR0001pG6qFP3NqAFi+3fg2XT2a4R50Hv8R/tw0+JbmoCk2CnI2mZzSweuKQNva9fhpxVqc+GgSxGOl5TLIjGSI4SqOtPG4ElgKNIdLQnPJ0A9sQ3fefLS9NgbtrjQ0RY8AspSDHYrWypVomZ6OYwUfom7DLAhaisQ+dDrskGbqzvIKam1jA4YV243Ksen182LeaLPzPxnjN/Q6P7IcnLvI/LyZd9Z+xlvH2bh3+Wr+2+J4zn3BXPi+4n67k3Md68UQLbpn2pXXpjro2+LQPPzNfD7achU9R3cZk41BGnkuwiyFS3m+DKJu93uiduG7ojrcLOreChXwUY4ygUkpguuZr3ewFh1TLYHnq2T1kIgLPxsy6+vXTKtJSMyqYCzjqMEwIztMN+/EFFN2xXpdzoqF9gX5U9IXTmJsQVmWIffIAUPeoUMf5O5PSFi0f2jwooOjw/O/LCpysf/7EgoKgll4/Bg2aMJI3cuxI81RUS8McTgiWLh12H8V/QMapl9cTQjPuQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Cancer data set pca inversion\"\n        title=\"\"\n        src=\"/static/ff88e4334e18ab03acbf3583cd2d5861/fcda8/pca_inversion.png\"\n        srcset=\"/static/ff88e4334e18ab03acbf3583cd2d5861/12f09/pca_inversion.png 148w,\n/static/ff88e4334e18ab03acbf3583cd2d5861/e4a3f/pca_inversion.png 295w,\n/static/ff88e4334e18ab03acbf3583cd2d5861/fcda8/pca_inversion.png 590w,\n/static/ff88e4334e18ab03acbf3583cd2d5861/3075e/pca_inversion.png 621w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>we obtain a very peculiar striped pattern. Intuitively, one would expect an almost noisy pattern due to the orthonormality of the singular vectors. To understand this we need to go back to some math. In the previous section we said that to calculate the principal components we substract the mean from the data set and obtain the principal component as <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>X</mi><mo>−</mo><msub><mi>μ</mi><mi>X</mi></msub><mo stretchy=\"false\">)</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">P = (X-\\mu_X) V^T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span></span>. Inverting this relation to calculate the data <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span> from a given principal component we obtain:</p>\n<div class=\"math math-display\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>X</mi><mtext> </mtext><mo>=</mo><mtext> </mtext><mi>P</mi><mi>V</mi><mo>+</mo><msub><mi>μ</mi><mi>X</mi></msub><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">X \\, = \\, PV + \\mu_X.</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">μ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">.</span></span></span></span></span></div>\n<p>For large means the data is dominated by those biased dimensions. To get a visual picture have a glance at the first figure of the post: The principal components are given by the black arrows and are clearly orthogonal. However if we shift the data by its mean the resulting arrows are the red ones which are clearly far from being orthogonal! Take-away: for large means the data is be goverened by it's biased even if it has a lot of variance.</p>\n<p>To get a read on the actual information that individual features contribute to the PCA's we normalize the data and look at the mean and variance of each feature contribution aggregated over all principal components<sup id=\"fnref-fn-dimensional-dependence\"><a href=\"#fn-fn-dimensional-dependence\" class=\"footnote-ref\">fn-dimensional-dependence</a></sup>.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5f5378fab160e4cf599f2477742a95eb/ca12d/pca_inversion_scaled.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.56756756756756%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsSAAALEgHS3X78AAACmElEQVR42pVTy04UQRRtBMUNcefKDzD+gGA00RgfMS5dOPyGK5SVX0OMkLg2JsQEhBBbJuhABobu6UdVd1X1u3uAmaKutwqY6EKjnZzcR9U991VtWZY1s7S0PJvnx28YKxdJlC9q+T8Qon7NefU2jssXyGdNdLu9ecBvNAJV1CN1fKJAjoz9TxgOldTxUsJ7TXjJtr+3RNIA5+UoIPmI0FxyVkm0pdaDMJOCVxKrGYP/qvPypK4kxlfvNOHk1pbdYqKEgArl+lz1QwFhlACJUvAIh303RimM7dOzs0ADdW2jXxb1EDyaLGvCK5/WNlqU5+ATpg48qtyAwb5HTYATxnDox+AEERIJ6BNmSDzK4MCLoOdH4JJIJuURuEG8Yir8vNZuMT4ASgsVhLlicQV+UAC2bUCo1muI0S94DYmGqIGSAmhUQipqWVUK+l5uZnjZttvzTDRw6AqFM1P9AFsLM/DOJaU5RFEBfT+FnisAZwpC6ES5uaPnWOAMHS81hNPtdqcV8wZCkqtEVAqlCdZwvAQrK7DiTJ8bElycASGaMNPnsqxOMdFZhVe3tzsvs+wIYlYqxiuFW8NLJbZTGAKs4CLQVKardb107LsgdPuJmeGUJkzTI32gMNgQahIt8bkYct3ahV8ncbF63xCWunKc4anegdnyFL7D+Qhb9gOhbPuH8jwGCb5L/APGwMrHOudndop3Ylwato8zPAWe1KblSU3IsxozZ2pnp6s6nZ7JnCSDMcnvCRpjc+Mf4OJyWTeACQaGcObVwsItFpb3imI4t7fnPnQcfr9p1OzGxrfHm5vbT3AUd9bXvz5dXf3yDAnmbLvzSOtNA7d3d50HKx8+Po/TwV0cyU3r/Juw/vxNI24grp/La3+L+QkbE3YPed4ECwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Average PCA contribution in scaled data set\"\n        title=\"\"\n        src=\"/static/5f5378fab160e4cf599f2477742a95eb/fcda8/pca_inversion_scaled.png\"\n        srcset=\"/static/5f5378fab160e4cf599f2477742a95eb/12f09/pca_inversion_scaled.png 148w,\n/static/5f5378fab160e4cf599f2477742a95eb/e4a3f/pca_inversion_scaled.png 295w,\n/static/5f5378fab160e4cf599f2477742a95eb/fcda8/pca_inversion_scaled.png 590w,\n/static/5f5378fab160e4cf599f2477742a95eb/ca12d/pca_inversion_scaled.png 647w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Given this result we could try to only keep features with large absolute mean and see how those perform in a modeling task. However, these ponderings are going beyond the scope of this blog. One could also think of other ways to analyze the contributions to the principal components -- playground for creativity ahead!</p>\n<p><strong>What's the deal?</strong></p>\n<p>In essence you should use PCA with caution and look at the outcomes and predictions of your PCA. If explained variance tells you that a single component is enough to explain everything you might have forgotten to normalize (maybe you don't want to normalize, though). Similar conclusion if your inversion is not orthonormal your data might be governed by large biases.</p>\n<p>Looking at PCAs and understanding them gives you insights into your data, especially if the dimensionality exceed simple EDA practices. Moreover, it gets your creative juices flowing of how to combine, engineer and/or eliminate features. The relevant code snippets can be found in my <a href=\"https://github.com/jotterbach/Data-Exploration-and-Numerical-Experimentation/blob/master/Data-Analytics/PCA_Pitfalls.ipynb\">github repo</a>. Enjoy playing!</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-fn-independence\">In fact only 49 states if we enforce indpendence, and there are no missing values.<a href=\"#fnref-fn-independence\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-fn-orthonormality\">Strictly speaking this is only true if <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span></span> is real and symmetric. For a matrix over the complex numbers <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">C</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{C}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6889em;\"></span><span class=\"mord mathbb\">C</span></span></span></span></span> we need to replace the transpose with the Hermitian conjugate.<a href=\"#fnref-fn-orthonormality\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-fn-dimensional-dependence\">Note that this quantity depends strongly on how many PCs are kept after dimensionality reduction.<a href=\"#fnref-fn-dimensional-dependence\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","title":"Principal Component Analysis (PCA) for Feature Selection and some of its Pitfalls"}},"staticQueryHashes":[],"slicesMap":{}}